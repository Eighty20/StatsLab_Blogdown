<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Enable responsiveness on mobile devices --> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    
    	
    <meta name="generator" content="Hugo 0.45.1" />
    
    <title>Predicting cloud movements &middot; </title>
    <meta content="Predicting cloud movements - " property="og:title">
    <meta content=" - " property="og:description">    
    <!-- CSS --> 
    <link rel="stylesheet" href="../../css/print.css" media="print">
    <link rel="stylesheet" href="../../css/poole.css">
    <link rel="stylesheet" href="../../css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,500">
    
    <script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
    <!-- highlight.js--> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <!-- Customised CSS -->
    <link rel="stylesheet" href="../../css/custom.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="../../favicon.png">
    

	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="../../"><h1 class="brand">StatsLab</h1></a>
			 <img src="../../img/stats.png" alt="Author Image" class="img-circle headshot center"> 
			<p class="lead">
				 Internal initiative at <br> <a href="http://www.eighty20.co.za/"> Eighty20</a> <br> We love R and Python 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="../../posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="../../about_me/"> <span>About</span></a>
				</li>
				<li>
					<a href="../../tags/"> <span>Search by Tag</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	
	
	
	
	
	
	
</section>

        </p>
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="post">
  <h1>Predicting cloud movements</h1>
  
  <div class="col-sm-12 col-md-12">
    <span class="text-left post-date meta">
            
       <i class="fa fa-calendar" aria-hidden="true"></i> Jul 7, 2018 
      
      
        
        
            in
            
            
                <a class="meta" href="../../categories/python">PYTHON</a>
                
            
        
      
      
      
        
        
            <br/>
             <i class="fa fa-tags" aria-hidden="true"></i>
            
            <a class="meta" href="../../tags/cnn">cnn</a> 
        
            <a class="meta" href="../../tags/deep_learning">deep_learning</a> 
        
            <a class="meta" href="../../tags/keras">keras</a> 
        
            <a class="meta" href="../../tags/lstm">lstm</a> 
        
            <a class="meta" href="../../tags/python">python</a> 
        
            <a class="meta" href="../../tags/time-series">time-series</a>
        
      
      
      </span>  
  </div>    
  
  

<p>In this notebook we will attempt to predict or forecast the movement of rain/clouds over the area of North America!</p>

<p>Of course the weather is a known chaotic system, therefore we will try to create an initial benchmark by throwing a deep learning model at the problem.</p>

<p>The images can be downloaded from AWS S3 using the following link <a href="https://s3-eu-west-1.amazonaws.com/data-problems/precipitation_data.zip">https://s3-eu-west-1.amazonaws.com/data-problems/precipitation_data.zip</a></p>

<h2 id="import-some-libraries">Import some libraries</h2>

<pre><code class="language-python">import os
from os import listdir
from PIL import Image as PImage
import matplotlib.pyplot as plt
import keras
import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.convolutional_recurrent import ConvLSTM2D
from keras.layers.normalization import BatchNormalization
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
import matplotlib.image as img
import PIL as Image
from keras.preprocessing.image import ImageDataGenerator
import sys
from keras.callbacks import TensorBoard
print(sys.version)
</code></pre>

<pre><code>3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 12:30:02) [MSC v.1900 64 bit (AMD64)]
</code></pre>

<h2 id="example-input">Example input</h2>

<p>The images downloaded from S3 will look like this:</p>

<pre><code class="language-python">path = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data/&quot;
path_out = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data_cleaned/&quot;

im = PImage.open(path+'20120830.gif').convert('RGB')
im
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_5_0.png" alt="png" /></p>

<h3 id="observations">Observations</h3>

<ul>
<li>Static images not animated even though they are .gif</li>
<li>CSV file containing the lag between the date of shown image and the date the image was taken. Not always consistent!</li>
<li>For image data deeplearning is almost surely the solution for the moment. Also we are dealing with stochastic datapoints so probably need to use a recurrent network.</li>
<li>These images have metadata in them we dont want. The index on the left and the text at the bottom might change and lead to false signal in the model</li>
<li>User of our model wants to provide images to it and get predictions. We need to have clear modules for processing input data for our model</li>
<li>Color channel of image input could be useful actually&hellip; We could remove anything that isnt moving from these images. Our predictions will only be clouds but whatever, should be easier to train and easy enough to put ontop of a geospatial shape file layer</li>
<li>Images are 400 by 320. We may want to reduce the size with our data import module</li>
<li>Since we will be predicting images not labels or scores we are building something similar to an autoencoder</li>
</ul>

<p>Based on a 2 second google search my assumptions were somewhat validated over here: <a href="http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/">http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/</a></p>

<p><strong>Possible insights</strong></p>

<ul>
<li>We could use dropin to perturb the input images beyond just data generators in keras. Instead of rotating the images we can duplicate certain sub frames to teach the network that time could have moved at different rates and still achieved same output image.</li>
<li>With images flowing in we could achieve this by duplicating images at random before creating our lagged prediction images</li>
</ul>

<p>I will have to edit these images so I need to remember how to use the PILLOW library: <a href="https://www.youtube.com/watch?v=6Qs3wObeWwc">https://www.youtube.com/watch?v=6Qs3wObeWwc</a></p>

<h2 id="clean-images-before-we-start">Clean images before we start</h2>

<p>This function will load all images in a folder and then clean all the pixels we aren&rsquo;t interested in</p>

<pre><code class="language-python">def cleanImages(path,new_path):
    # return array of images

    imagesList = listdir(path)
    for image in imagesList:
        if(image.endswith('.gif')):
            fname,fext = os.path.splitext(image)
            im = PImage.open(path + image).convert('RGB')
            width, _ = im.size
            for i, px in enumerate(im.getdata()):
                y = i / width
                x = i % width
                if (px == (0,0,0)):
                    im.putpixel((x, y), (255,255,255))
                if ((x&lt;4*400/100)or(_*0.88&lt;y)):
                    im.putpixel((x, y), (255,255,255))
            im.save(new_path+image)          
</code></pre>

<p>The function is very simple. The function will remove all black pixels from the image (the shapefiles) and also crop away the legend and sub title of the images that we are not interested in.</p>

<p>This is what the output images will look like:</p>

<pre><code class="language-python">im = PImage.open(path_out+'20120830.gif')
im
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_10_0.png" alt="png" /></p>

<p>Since we are going to use a deep learning model to do the predictions we don&rsquo;t need to do too much feature engineering. For now this will be enough to server our purposes.</p>

<p>Now it&rsquo;s time to get the data ready for a deep learning model.</p>

<h2 id="deep-learning-model">Deep learning model</h2>

<h3 id="choice-of-model">Choice of model</h3>

<p>For our final model we will almost surely need to use a recurrent neural network. The reason is simply that we are trying to predict using stochastic data. We also need to consider that we are dealing with image data. One of the best network architectures for images convolutional network layers.</p>

<p>For our initial test we can simply build a convolutional network to train on the data. This help formalize the data preparation process</p>

<h3 id="convolutional-network">Convolutional network</h3>

<p>For a convolutional network we need to provide a 3 dimensional tensor where our shape is defined (samples,width,height,channels), where channels depict the RGB color value of each pixel.</p>

<p>Matplotlib.img has a useful tool for reading in images as unmpy arrays. Each of these imported images will be of shape (width,height,channels). To prepare for our convolutional network we need to have another dimension for the samples.</p>

<p>Note that the convention is to use img_to_array(load_img(new_path+image)) using the keras library. This is just a bit more vebose than what I used. This workflow can be more powerful if we intend to use generators to apply transformations to the immages.</p>

<p>We produce the data using a simple list comprehension:</p>

<pre><code class="language-python">%%time
new_path = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data_cleaned_jpeg/&quot;
path_orig = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data/&quot;
path_cleaned = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data_cleaned/&quot;

imagesList = os.listdir(new_path)

training_data = np.array( [img.imread(new_path+image) for image in imagesList])
</code></pre>

<pre><code>Wall time: 5.35 s
</code></pre>

<p>This takes about 5 seconds because we have quite a few images in here:</p>

<pre><code class="language-python">len(imagesList)
</code></pre>

<pre><code>2383
</code></pre>

<p>Let&rsquo;s make sure we have the right shape for our 4D tensor:</p>

<pre><code class="language-python">training_data.shape
</code></pre>

<pre><code>(2383, 320, 400, 3)
</code></pre>

<p>Before we throw these numpy arrays at our deep learning model it is important to scale our inputs between [0,1]. The obvious reason is that standard convention to scale inputs helps against exploding model weights. In our case it is also important because we will use a crossentropy method to measure the accuracy of our model. This wouldn&rsquo;t make sense if our values weren&rsquo;t between [0,1].</p>

<p>It&rsquo;s also a good idea to make sure your values are float32 (or float16 when working with a GPU that has tensorcores)</p>

<p>Let&rsquo;s scale this 4D tensor:</p>

<pre><code class="language-python">training_data = np.divide(training_data,255)
</code></pre>

<p>The time interval that we have images for is Jan 2012 to 10 July 2018 (the time I downloaded the images):</p>

<pre><code class="language-python">print(imagesList[0],imagesList[-1])
</code></pre>

<pre><code>20120101.jpg 20180710.jpg
</code></pre>

<p>We can also see that we have one image for each day:</p>

<pre><code class="language-python">imagesList[0:5]
</code></pre>

<pre><code>['20120101.jpg',
 '20120102.jpg',
 '20120103.jpg',
 '20120104.jpg',
 '20120105.jpg']
</code></pre>

<p>For our first network we will simply predict the next day&rsquo;s satellite image. To do this we set the input X to be the 1 day lagged images:</p>

<pre><code class="language-python">train_len = int(np.floor(len(training_data)*0.75))
test_len = int(np.floor((len(training_data)-train_len) * 0.5))
val_len = int(len(training_data)-test_len-train_len)

print(train_len,test_len,val_len)
</code></pre>

<pre><code>1787 298 298
</code></pre>

<pre><code class="language-python">train_split = training_data[0:train_len]
test_split = training_data[train_len:train_len+test_len]
val_split = training_data[train_len+test_len:len(training_data)]

print(len(train_split),len(test_split),len(val_split))
</code></pre>

<pre><code>1787 298 298
</code></pre>

<pre><code class="language-python">X = train_split[0:len(train_split)-1,:,:,:]
Y = train_split[1:len(train_split),:,:,:]

X_val = val_split[0:len(val_split)-1,:,:,:]
y_val = val_split[1:len(val_split),:,:,:]
</code></pre>

<pre><code class="language-python">print(len(X),len(Y),len(X_val),len(y_val))
</code></pre>

<pre><code>1786 1786 297 297
</code></pre>

<p>We split off some data for validating our training and we also split off some data to validate our model after we have tuned the parameters. The test data is important if we intend to tune the model often to maximise our validation accuracy. After a few iterations we are leaking some knowledge about the preformance of the model on the validation data.</p>

<h3 id="define-model">Define model</h3>

<p>For our initial model I define a simple stacked network of 3 convolutional layers. Normally we would use layers like max pooling to reduce the complexity of our model. This won&rsquo;t be needed because we are not going to use all too many layers and we are predicting images of the same size; max pooling will extract and combine local features but then we would need to widen our units in the layers to get back to the predicted image.</p>

<p>To use max_pooling we would use MaxPooling2D((2,2)) and UpSampling2D((2,2)) to return to the same dimensions</p>

<pre><code class="language-python">conv_model = Sequential()
conv_model.add(Conv2D(3,(3,3),input_shape=(320, 400, 3), activation='relu',padding='same'))
conv_model.add(Conv2D(3,(3,3),input_shape=(320, 400, 3), activation='relu',padding='same'))
conv_model.add(Conv2D(3,(3,3),input_shape=(320, 400, 3), activation='relu',padding='same'))

conv_model.summary()
</code></pre>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_10 (Conv2D)           (None, 320, 400, 3)       84        
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 320, 400, 3)       84        
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 320, 400, 3)       84        
=================================================================
Total params: 252
Trainable params: 252
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>For this kind of prediction I will use an optimizer very similar to the optimizers used for auto encoding. Since each pixel in our model is now scaled between [0,1] instead of [0,255] we can use a binary_crossentropy to optimize our fit:</p>

<pre><code class="language-python">conv_model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
#               optimizer='adadelta'
#               metrics=['accuracy']
                  )
</code></pre>

<pre><code class="language-python">history = conv_model.fit(X, Y,
                epochs=10,
                validation_data=(X_val, y_val)
#                 callbacks=[TensorBoard(log_dir='C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/tensorboard')]
               )
</code></pre>

<pre><code>Train on 1786 samples, validate on 297 samples
Epoch 1/10
1786/1786 [==============================] - 15s - loss: 2.2330 - val_loss: 0.5243
Epoch 2/10
1786/1786 [==============================] - 15s - loss: 0.3955 - val_loss: 0.3257
Epoch 3/10
1786/1786 [==============================] - 15s - loss: 0.3392 - val_loss: 0.3122
Epoch 4/10
1786/1786 [==============================] - 15s - loss: 0.3324 - val_loss: 0.3040
Epoch 5/10
1786/1786 [==============================] - 15s - loss: 0.3267 - val_loss: 0.3057
Epoch 6/10
1786/1786 [==============================] - 15s - loss: 0.3213 - val_loss: 0.2986
Epoch 7/10
1786/1786 [==============================] - 15s - loss: 0.3129 - val_loss: 0.2949
Epoch 8/10
1786/1786 [==============================] - 15s - loss: 0.3121 - val_loss: 0.2893
Epoch 9/10
1786/1786 [==============================] - 15s - loss: 0.3126 - val_loss: 0.2838
Epoch 10/10
1786/1786 [==============================] - 15s - loss: 0.3157 - val_loss: 0.2892
</code></pre>

<p>Plot the loss:</p>

<pre><code class="language-python">plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_36_0.png" alt="png" /></p>

<p>So we can see the we don&rsquo;t seem to have issues of over fitting the data. This isn&rsquo;t really my concern right now since we first need to achieve a model able to predict future movements of clouds. So on that note, let&rsquo;s investigate the output of the model more closely:</p>

<h4 id="example-input-1">Example input</h4>

<pre><code class="language-python">imgplot = plt.imshow(X[0])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_39_0.png" alt="png" /></p>

<h4 id="example-predicted-next-image">Example predicted next image:</h4>

<pre><code class="language-python">predict_img = conv_model.predict(X[0:5])

imgplot = plt.imshow(predict_img[0])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_41_0.png" alt="png" /></p>

<h4 id="what-really-happened">What really happened</h4>

<pre><code class="language-python">real_img = Y[0:5]
imgplot = plt.imshow(real_img[0])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_43_0.png" alt="png" /></p>

<p>So our naive model hasn&rsquo;t really learned anything impressive</p>

<h3 id="convolutional-lstm-network">Convolutional LSTM network</h3>

<pre><code class="language-python">new_path = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data_cleaned_jpeg/&quot;
path_orig = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data/&quot;
path_cleaned = &quot;C:/Users/User-pc/Desktop/Work/Git_Repositories/Precipitation_Forecast/Data/precipitation_data/data_cleaned/&quot;

imagesList = os.listdir(new_path)

# training_data = np.array( [img.imread(new_path+image) for image in imagesList])
training_data = np.array([img_to_array(load_img(new_path+image)) for image in imagesList])

</code></pre>

<pre><code class="language-python">training_data = np.divide(training_data,255)
</code></pre>

<p>For this recurrent structure I want to go up from a 4D tensor input to a 5D tensor. The new dimension is the time index.</p>

<p>Theoretically that means having observations where each observation is a sequence of images. In our case an observation should be a week of images, such that we are in fact predicting the next week of images.</p>

<pre><code class="language-python">train_split = training_data[0:train_len]
test_split = training_data[train_len:train_len+test_len]
val_split = training_data[train_len+test_len:len(training_data)]
</code></pre>

<pre><code class="language-python">len(train_split)%7
</code></pre>

<pre><code>2
</code></pre>

<p>Currently we will have 2 days that do not fit into a full week. For now we will simply remove these images:</p>

<pre><code class="language-python">train_split = np.delete(train_split,[i for i in range(len(train_split)%7)],axis=0)
test_split = np.delete(test_split,[i for i in range(len(test_split)%7)],axis=0)
val_split = np.delete(val_split,[i for i in range(len(val_split)%7)],axis=0)
</code></pre>

<p>Now that we have full weeks in our data we need to stack these 4D tensors for our sequence dimension:</p>

<pre><code class="language-python">%%time
week_data_train = []
while(len(train_split)&gt;6):
    week = np.array([train_split[k] for k in range(7)])
    week_data_train.append(week)
    train_split = np.delete(train_split,[i for i in range(7)],axis=0)
    
week_data_train = np.array(week_data_train)
</code></pre>

<pre><code class="language-python">%%time
week_data_test = []
while(len(test_split)&gt;6):
    week = np.array([test_split[k] for k in range(7)])
    week_data_test.append(week)
    test_split = np.delete(test_split,[i for i in range(7)],axis=0)
    
week_data_test = np.array(week_data_test)
</code></pre>

<pre><code>Wall time: 4.56 s
</code></pre>

<p>We can verify that we now have a 5D tensor to train on:</p>

<pre><code class="language-python">week_data_train.shape
</code></pre>

<pre><code>(255, 7, 320, 400, 3)
</code></pre>

<p>So we can train on 255 full weeks worth of data</p>

<pre><code class="language-python">X = week_data_train[0:len(week_data_train)-1]
Y = week_data_train[1:len(week_data_train)]

X_val = week_data_test[0:len(week_data_test)-1]
y_val = week_data_test[1:len(week_data_test)]

</code></pre>

<h4 id="define-network">Define network</h4>

<p>For this recurrent network we still want to use convolutional layers so I will use the handy keras layer called ConvLSTM2D. It will train a long short term memory network using convolutions. We will also specify go_backwards=True in order to use a bidirectional recurrent layout.</p>

<pre><code class="language-python">conv_model = Sequential()
conv_model.add(ConvLSTM2D(3,kernel_size=(3,3),input_shape=(7,320, 400, 3),padding='same', return_sequences=True,go_backwards=True))
conv_model.add(ConvLSTM2D(3,kernel_size=(3,3),padding='same', return_sequences=True,go_backwards=True))

conv_model.summary()
</code></pre>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv_lst_m2d_5 (ConvLSTM2D)  (None, 7, 320, 400, 3)    660       
_________________________________________________________________
conv_lst_m2d_6 (ConvLSTM2D)  (None, 7, 320, 400, 3)    660       
=================================================================
Total params: 1,320
Trainable params: 1,320
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<pre><code class="language-python">conv_model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
#               metrics=['accuracy']
                  )
</code></pre>

<p>Here I use a batch_size of 8 because any larger gave me out of memory errors.</p>

<pre><code class="language-python">%%time
history = conv_model.fit(X, Y,
                epochs=10,
                batch_size = 8,
                shuffle = False
                ,validation_data=(X_val, y_val)
               )
</code></pre>

<pre><code>Train on 254 samples, validate on 41 samples
Epoch 1/10
254/254 [==============================] - 46s - loss: 5.5493 - val_loss: 4.9973
Epoch 2/10
254/254 [==============================] - 44s - loss: 3.3088 - val_loss: 1.3253
Epoch 3/10
254/254 [==============================] - 44s - loss: 0.9746 - val_loss: 0.5314
Epoch 4/10
254/254 [==============================] - 44s - loss: 0.4552 - val_loss: 0.4045
Epoch 5/10
254/254 [==============================] - 44s - loss: 0.3732 - val_loss: 0.3543
Epoch 6/10
254/254 [==============================] - 44s - loss: 0.3412 - val_loss: 0.3352
Epoch 7/10
254/254 [==============================] - 43s - loss: 0.3291 - val_loss: 0.3270
Epoch 8/10
254/254 [==============================] - 43s - loss: 0.3241 - val_loss: 0.3221
Epoch 9/10
254/254 [==============================] - 43s - loss: 0.3208 - val_loss: 0.3268
Epoch 10/10
254/254 [==============================] - 43s - loss: 0.3190 - val_loss: 0.3192
Wall time: 7min 24s
</code></pre>

<p>Note that we specify Shuffle = False since the sequence is important</p>

<pre><code class="language-python">conv_model.save('CNN_LSTM_bidirectional_10epochs.h5')
</code></pre>

<pre><code class="language-python">plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
# plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_67_0.png" alt="png" /></p>

<pre><code class="language-python">predict_img = conv_model.predict(X[0:1])
real_img = Y[0:1]
</code></pre>

<p>So our first week of images were:</p>

<pre><code class="language-python">fig=plt.figure(figsize=(320, 400))
for i in range(1, 7 +1):
#     img = plt.imshow()
    fig.add_subplot(8, 1, i)
    plt.imshow(X[0,i-1])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_70_0.png" alt="png" /></p>

<p>And we predicted this would happen:</p>

<pre><code class="language-python">fig=plt.figure(figsize=(320, 400))
for i in range(1, 7 +1):
#     img = plt.imshow()
    fig.add_subplot(8, 1, i)
    plt.imshow(predict_img[0,i-1])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_72_0.png" alt="png" /></p>

<p>This is actually very promising&hellip; Our model is starting to make some novel predictions in new areas.</p>

<p>This is what we were suppose to see:</p>

<pre><code class="language-python">fig=plt.figure(figsize=(320, 400))
for i in range(1, 7 +1):
#     img = plt.imshow()
    fig.add_subplot(8, 1, i)
    plt.imshow(real_img[0,i-1])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_74_0.png" alt="png" /></p>

<h4 id="zoom-in-on-final-image">Zoom in on final image:</h4>

<p><strong>Input</strong></p>

<pre><code class="language-python">imgplot = plt.imshow(X[0,6])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_76_0.png" alt="png" /></p>

<p><strong>Predicted</strong></p>

<pre><code class="language-python">imgplot = plt.imshow(predict_img[0,6])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_78_0.png" alt="png" /></p>

<p><strong>Actual future</strong></p>

<pre><code class="language-python">imgplot = plt.imshow(real_img[0,6])
plt.show()
</code></pre>

<p><img src="../../Predicting_cloud_movements_files/Predicting_cloud_movements_80_0.png" alt="png" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>My initial impression is that the current model really struggles to predict anything other than what it sees right now. Even so with our reccurrent network we started seeing some novel predictions.</p>

<p>All in all I&rsquo;m not too surprised! The data we gave the model is only 1 image per day. Perhaps if we had access to 24 images per day the hour on hour move of pixels would be more informative.</p>

<p>The model does not yet have any context for what the color of the pixels may be in the future either.</p>

<p>Perhaps with a scaled up version of the model and more training we will see a better prediction. But more likely we need to start creating our own network graphs and loss functions. Currently we are optimizing mean binary crossentropy over all the pixels - a better loss function will measure binary crossentropy over a max_pooling of the output. Perhaps we can even define a pooling of pixels that uses a MAX instead of an average to help against all the empty space.</p>

</div>
            <div class="footer">
                <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>


        <h2>Comments</h2>
        <div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://statslab.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

            </div>
        </div>
        
                
    </body>
</html>
