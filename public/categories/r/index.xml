<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on </title>
    <link>/categories/r/</link>
    <description>Recent content in R on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 28 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dealing with nested data</title>
      <link>/posts/using_e-sports_api/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using_e-sports_api/</guid>
      <description>Dealing with nested data can be really frustrating…
Especially if you want to keep your workspace nice and tidy with all your data in tables!
With no actual experience trying to get at these nested tibbles can seem almost impossible:
via GIPHY
--  Downloading data from an api created by Blizzard To illustrate how you would deal with nested data I found an api that let’s you download all kinds of data on the e-sport/game called Overwatch.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/posts/data-wrangling/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-wrangling/</guid>
      <description>Why data-wrangling If you can wrangle data into the proper form you can do anything with it…
 Data-wrangling is absolutely essential for every data science task where we need to work with collected data.
 A recent article from the New York Times said “Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information.</description>
    </item>
    
    <item>
      <title>Benchmarking machine learning models in parallel</title>
      <link>/posts/benchmarking_in_parallel/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/benchmarking_in_parallel/</guid>
      <description>Overview Having just started playing with deeplearning models in R, I wanted to visually compare them to other more traditional ML workflows. Of course, deeplearning is generally used where other models fail, but with no need for feature selection and rapidly increasing power and ease of use they may just evolve into a general learning paradigm.
 However, with tabular data and packages like caret the machine learning methods have become so streamlined that minimal user input is required at all.</description>
    </item>
    
    <item>
      <title>A Soft Introduction to Machine Learning</title>
      <link>/posts/intro-machine-learning/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/intro-machine-learning/</guid>
      <description>Important machine learning libraries Future additions  Part 1 - Data Preprocessing Part 2 - Regression Simple Linear Regression Multiple linear regression Polynomial Regression Support vector machine regression Regression Trees Random forest regression A more robust application of machine learning regressions (random forest)  Part 3 - Clustering K-means  Part 4 - Dimensionality Reduction Create PCA  Part 5 - Reinforced Learning Multi-Armed Bandit Problem Upper Confidence Bound (UCB) method Improve results using UCB Visualize the model add selection  Part 6 - Parameter Grid Search, Cross-validation and Boosting Grid Search and Parameter Tuning XGBoost    Important machine learning libraries I created this document to serve as an easy introduction to basic machine learning methods.</description>
    </item>
    
  </channel>
</rss>