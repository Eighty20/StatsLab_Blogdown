<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 28 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dealing with nested data</title>
      <link>/posts/using_e-sports_api/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using_e-sports_api/</guid>
      <description>Dealing with nested data can be really frustrating…
Especially if you want to keep your workspace nice and tidy with all your data in tables!
With no actual experience trying to get at these nested tibbles can seem almost impossible:
via GIPHY
--  Downloading data from an api created by Blizzard To illustrate how you would deal with nested data I found an api that let’s you download all kinds of data on the e-sport/game called Overwatch.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/posts/data-wrangling/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-wrangling/</guid>
      <description>Why data-wrangling If you can wrangle data into the proper form you can do anything with it…
 Data-wrangling is absolutely essential for every data science task where we need to work with collected data.
 A recent article from the New York Times said “Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information.</description>
    </item>
    
    <item>
      <title>Data Science Workflow</title>
      <link>/posts/data-science-workflow/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-science-workflow/</guid>
      <description>Why it matters When working with recurring clients and projects together with tight deadlines it is easy to cut corners or forget good practice.
Here are the top reasons why a data science workflow is vital:
 Work is well documented
 Folders and files logically are structured
 Projects remain version controlled
 Data output is tracked
 Logical seperation of scripts make productizing easy
 Good naming conventions make tracking of the project flow easy</description>
    </item>
    
    <item>
      <title>Benchmarking machine learning models in parallel</title>
      <link>/posts/benchmarking_in_parallel/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/benchmarking_in_parallel/</guid>
      <description>Overview Having just started playing with deeplearning models in R, I wanted to visually compare them to other more traditional ML workflows. Of course, deeplearning is generally used where other models fail, but with no need for feature selection and rapidly increasing power and ease of use they may just evolve into a general learning paradigm.
 However, with tabular data and packages like caret the machine learning methods have become so streamlined that minimal user input is required at all.</description>
    </item>
    
    <item>
      <title>Design and Analysis of Experiments with R</title>
      <link>/posts/design_and_analysis_of_experiments_with_r/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/design_and_analysis_of_experiments_with_r/</guid>
      <description>Introduction This document will cover in depth the use of DoE experiments in R! We will focus our attention to measuring treatment effects, not variances.
Based on the work:
Lawson, John. Design and Analysis of Experiments with R. Chapman and Hall/CRC, 20141217. VitalBook file.
  Definitions  Experiment (also called a Run) is an action where the experimenter changes at least one of the variables being studied and then observes the effect of his or her actions(s).</description>
    </item>
    
    <item>
      <title>A Soft Introduction to Machine Learning</title>
      <link>/posts/intro-machine-learning/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/intro-machine-learning/</guid>
      <description>Important machine learning libraries Future additions  Part 1 - Data Preprocessing Part 2 - Regression Simple Linear Regression Multiple linear regression Polynomial Regression Support vector machine regression Regression Trees Random forest regression A more robust application of machine learning regressions (random forest)  Part 3 - Clustering K-means  Part 4 - Dimensionality Reduction Create PCA  Part 5 - Reinforced Learning Multi-Armed Bandit Problem Upper Confidence Bound (UCB) method Improve results using UCB Visualize the model add selection  Part 6 - Parameter Grid Search, Cross-validation and Boosting Grid Search and Parameter Tuning XGBoost    Important machine learning libraries I created this document to serve as an easy introduction to basic machine learning methods.</description>
    </item>
    
    <item>
      <title>About Hugo</title>
      <link>/about/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hugo is a static site engine written in Go.
It makes use of a variety of open source projects including:
 Cobra Viper J Walter Weatherman Cast  Learn more and contribute on GitHub.</description>
    </item>
    
  </channel>
</rss>