<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Enable responsiveness on mobile devices --> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    
    	
    <meta name="generator" content="Hugo 0.44" />
    
    <title>IMDB movie classification &middot; </title>
    <meta content="IMDB movie classification - " property="og:title">
    <meta content=" - " property="og:description">    
    <!-- CSS --> 
    <link rel="stylesheet" href="../../css/print.css" media="print">
    <link rel="stylesheet" href="../../css/poole.css">
    <link rel="stylesheet" href="../../css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,500">
    
    <script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
    <!-- highlight.js--> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <!-- Customised CSS -->
    <link rel="stylesheet" href="../../css/custom.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="../../favicon.png">
    

	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="../../"><h1 class="brand">StatsLab</h1></a>
			 <img src="../../img/stats.png" alt="Author Image" class="img-circle headshot center"> 
			<p class="lead">
				 Internal initiative at <br> <a href="http://www.eighty20.co.za/"> Eighty20</a> <br> We love R and Python 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="../../posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="../../about_me/"> <span>About</span></a>
				</li>
				<li>
					<a href="../../tags/"> <span>Search by Tag</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	
	
	
	
	
	
	
</section>

        </p>
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="post">
  <h1>IMDB movie classification</h1>
  
  <div class="col-sm-12 col-md-12">
    <span class="text-left post-date meta">
            
       <i class="fa fa-calendar" aria-hidden="true"></i> Jul 7, 2018 
      
      
        
        
            in
            
            
                <a class="meta" href="../../categories/r">R</a>
                
            
        
      
      
      
        
        
            <br/>
             <i class="fa fa-tags" aria-hidden="true"></i>
            
            <a class="meta" href="../../tags/classification">classification</a> 
        
            <a class="meta" href="../../tags/deep_learning">deep_learning</a> 
        
            <a class="meta" href="../../tags/keras">keras</a>
        
      
      
      </span>  
  </div>    
  
  <div id="TOC">
<ul>
<li><a href="#overview">Overview</a><ul>
<li><a href="#load-the-imdb-data">Load the IMDB data</a><ul>
<li><a href="#view-data">View data</a></li>
<li><a href="#link-the-original-data">Link the original data</a></li>
</ul></li>
<li><a href="#prepare-data-as-tensors">Prepare data as tensors</a><ul>
<li><a href="#one-hot-encode">One-hot encode</a></li>
<li><a href="#set-outcome-data-types">Set outcome data types</a></li>
</ul></li>
<li><a href="#build-network">Build network</a></li>
<li><a href="#split-testtrain">Split test/train</a></li>
<li><a href="#train">Train</a><ul>
<li><a href="#if-we-train-for-only-4-epochs">If we train for only 4 epochs</a></li>
<li><a href="#if-we-use-dropout">If we use dropout</a></li>
</ul></li>
<li><a href="#investigate-the-best-predicted-movie">Investigate the best predicted movie</a></li>
</ul></li>
</ul>
</div>

<div id="overview" class="section level1">
<h1>Overview</h1>
<p>This post follows through the example in the book “Deep learning with R” by Francios Chollet with J. J. Alaire.</p>
<p><strong>Can a machine predict a good or bad movie rating?</strong></p>
<div class="figure">
<img src="../../Pictures/IMDB_keras/header.jpeg" />

</div>
<div id="load-the-imdb-data" class="section level2">
<h2>Load the IMDB data</h2>
<pre class="r"><code>imdb &lt;- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %&lt;-% imdb</code></pre>
<div id="view-data" class="section level3">
<h3>View data</h3>
<pre class="r"><code>train_data[[1]]</code></pre>
<pre><code>##   [1]    1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941
##  [15]    4  173   36  256    5   25  100   43  838  112   50  670    2    9
##  [29]   35  480  284    5  150    4  172  112  167    2  336  385   39    4
##  [43]  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147
##  [57] 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16
##  [71]   43  530   38   76   15   13 1247    4   22   17  515   17   12   16
##  [85]  626   18    2    5   62  386   12    8  316    8  106    5    4 2223
##  [99] 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25
## [113]  124   51   36  135   48   25 1415   33    6   22   12  215   28   77
## [127]   52    5   14  407   16   82    2    8    4  107  117 5952   15  256
## [141]    4    2    7 3766    5  723   36   71   43  530  476   26  400  317
## [155]   46    7    4    2 1029   13  104   88    4  381   15  297   98   32
## [169] 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476
## [183]   26  480    5  144   30 5535   18   51   36   28  224   92   25  104
## [197]    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113
## [211]  103   32   15   16 5345   19  178   32</code></pre>
<p>Here we can see the feature engineered dataset ready for the model. Each word from the original review corpus has been encoded and given a number. So each number represents a word.</p>
<p>There are different approaches we could’ve taken, for example building a word dictionary and counting the number of occurences of each word in the dictionary within each review. Here we simply encode all the words in the reviews to numbers.</p>
<p>Using word counts and shallow learning tecniques like non-negative matrix factorization can work but you end up dealing with very sparse data and increasingly large matrices. By encoding the words as numbers we can use deep neural networks to embed the information into more succinct dimensional space to train on without having to deal with the size and sparsity.</p>
<pre class="r"><code>train_labels %&gt;% head</code></pre>
<pre><code>## [1] 1 0 0 1 0 0</code></pre>
<p>And as our labels we have the binary good/bad outcome…</p>
</div>
<div id="link-the-original-data" class="section level3">
<h3>Link the original data</h3>
<p>We can link back the original words using the following data:</p>
<pre class="r"><code>word_index &lt;- dataset_imdb_word_index()
reverse_word_index &lt;- names(word_index)
names(reverse_word_index) &lt;- word_index
decoded_review &lt;- sapply(train_data[[1]], function(index) {
  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else &quot;?&quot;
})</code></pre>
<p>Or we can make a function that will decode the input for us:</p>
<pre class="r"><code>decode_words &lt;- function(word_counts){
    
    sapply(word_counts, function(index) {
  word &lt;- if (index &gt;= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else &quot;?&quot;
    })
}</code></pre>
<p>Let’s try:</p>
<pre class="r"><code>train_data[[1]] %&gt;% 
    decode_words %&gt;% 
    head</code></pre>
<pre><code>## [1] &quot;?&quot;         &quot;this&quot;      &quot;film&quot;      &quot;was&quot;       &quot;just&quot;      &quot;brilliant&quot;</code></pre>
</div>
</div>
<div id="prepare-data-as-tensors" class="section level2">
<h2>Prepare data as tensors</h2>
<p>For this text data we could simply have a 2D tensor where the first axis is our batches/movies and the second axis is each one of the possible words. In this case we restricted it to the top 10000 words in the corpus so we will have 10000 columns in this 2D tensor.</p>
<p>Currently however all our reviews have different lengths because they do not all have every possible word in them:</p>
<pre class="r"><code>train_data %&gt;%
    map_int(length) %&gt;%
    head</code></pre>
<pre><code>## [1] 218 189 141 550 147  43</code></pre>
<p>In order for us to create a 2D tensor we can either pad each of these elements or one-hot encode them to 1/0 so that we have [samples, 10000]</p>
<div id="one-hot-encode" class="section level3">
<h3>One-hot encode</h3>
<p>We can use the built in keras function:</p>
<pre class="r"><code>train_data %&lt;&gt;%
        map(~.x %&gt;% keras::k_one_hot(num_classes = 10000))</code></pre>
<p>Or if we did it manually:</p>
<pre class="r"><code>vectorize_sequences &lt;- function(sequences, dimension = 10000) {
  results &lt;- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] &lt;- 1
  results
}

x_train &lt;- vectorize_sequences(train_data)
x_test &lt;- vectorize_sequences(test_data)

x_train[1:10,1:10]</code></pre>
<pre><code>##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
##  [1,]    1    1    0    1    1    1    1    1    1     0
##  [2,]    1    1    0    1    1    1    1    1    1     0
##  [3,]    1    1    0    1    0    1    1    1    1     0
##  [4,]    1    1    0    1    1    1    1    1    1     1
##  [5,]    1    1    0    1    1    1    1    1    0     1
##  [6,]    1    1    0    1    0    0    0    1    0     1
##  [7,]    1    1    0    1    1    1    1    1    1     0
##  [8,]    1    1    0    1    0    1    1    1    1     1
##  [9,]    1    1    0    1    1    1    1    1    1     0
## [10,]    1    1    0    1    1    1    1    1    1     1</code></pre>
</div>
<div id="set-outcome-data-types" class="section level3">
<h3>Set outcome data types</h3>
<p>Currently the outcomes are integers but we want them as numeric R types:</p>
<pre class="r"><code>y_train &lt;- as.numeric(train_labels)
y_test &lt;- as.numeric(test_labels)</code></pre>
</div>
</div>
<div id="build-network" class="section level2">
<h2>Build network</h2>
<pre class="r"><code>model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 1, activation = &quot;sigmoid&quot;)

model %&gt;% compile(
  optimizer = optimizer_rmsprop(lr = 0.001),
  loss = loss_binary_crossentropy,
  metrics = metric_binary_accuracy
)</code></pre>
</div>
<div id="split-testtrain" class="section level2">
<h2>Split test/train</h2>
<pre class="r"><code>val_indices &lt;- 1:10000

x_val &lt;- x_train[val_indices,]
partial_x_train &lt;- x_train[-val_indices,]
y_val &lt;- y_train[val_indices]
partial_y_train &lt;- y_train[-val_indices]</code></pre>
</div>
<div id="train" class="section level2">
<h2>Train</h2>
<pre class="r"><code>history &lt;- model %&gt;% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  # batch_size = 512,
  validation_data = list(x_val, y_val)
)

history %&gt;% plot</code></pre>
<p><img src="../../posts/Classifying_IMDB_ratings_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>This shows a very interesting inverse trend between the training and the validation accuracy. This is better known as over-fitting.</p>
<div id="if-we-train-for-only-4-epochs" class="section level3">
<h3>If we train for only 4 epochs</h3>
<pre class="r"><code>model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;%
  layer_dense(units = 1, activation = &quot;sigmoid&quot;)

model %&gt;% compile(
  optimizer = &quot;rmsprop&quot;,
  loss = &quot;binary_crossentropy&quot;,
  metrics = c(&quot;accuracy&quot;)
)

model %&gt;% fit(x_train, y_train, epochs = 4, batch_size = 512)
results &lt;- model %&gt;% evaluate(x_test, y_test)

results %&gt;% head</code></pre>
<pre><code>## $loss
## [1] 0.2899548
## 
## $acc
## [1] 0.8854</code></pre>
</div>
<div id="if-we-use-dropout" class="section level3">
<h3>If we use dropout</h3>
<pre class="r"><code>model &lt;- keras_model_sequential() %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;, input_shape = c(10000)) %&gt;%
  layer_dense(units = 16, activation = &quot;relu&quot;) %&gt;%
    layer_dropout(rate = 0.3) %&gt;% 
  layer_dense(units = 1, activation = &quot;sigmoid&quot;)

model %&gt;% compile(
  optimizer = &quot;rmsprop&quot;,
  loss = &quot;binary_crossentropy&quot;,
  metrics = c(&quot;accuracy&quot;)
)

history &lt;- model %&gt;% fit(x_train, y_train, epochs = 40, batch_size = 256,validation_data = list(x_val, y_val))

history %&gt;% plot</code></pre>
<p><img src="../../posts/Classifying_IMDB_ratings_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>results &lt;- model %&gt;% evaluate(x_test, y_test)

results %&gt;% head</code></pre>
<pre><code>## $loss
## [1] 1.589528
## 
## $acc
## [1] 0.84872</code></pre>
<p>We can see that using things like dropout can correct the overfitting problem.</p>
</div>
</div>
<div id="investigate-the-best-predicted-movie" class="section level2">
<h2>Investigate the best predicted movie</h2>
<p>So according to the model we can see the ratings by predicting the outcome:</p>
<pre class="r"><code>options(scipen = 999)
predictions &lt;- 
    model %&gt;% predict(x_test) 

predictions %&gt;% head</code></pre>
<pre><code>##                             [,1]
## [1,] 0.9998654127120971679687500
## [2,] 0.0000000000000000002472396
## [3,] 1.0000000000000000000000000
## [4,] 0.0000003745669516774796648
## [5,] 0.0188427586108446121215820
## [6,] 0.0000000379938214223329851</code></pre>
<p>We see movie 3 in the test set recieved a perfect review from our super objective model here…</p>
<p>Let’s see what the model learned from other people to arrive at this rating:</p>
<pre class="r"><code>test_data[[3]] %&gt;% decode_words</code></pre>
<pre><code>##   [1] &quot;?&quot;              &quot;kate&quot;           &quot;played&quot;         &quot;by&quot;            
##   [5] &quot;one&quot;            &quot;of&quot;             &quot;the&quot;            &quot;most&quot;          
##   [9] &quot;nominated&quot;      &quot;actresses&quot;      &quot;of&quot;             &quot;the&quot;           
##  [13] &quot;last&quot;           &quot;decade&quot;         &quot;of&quot;             &quot;this&quot;          
##  [17] &quot;century&quot;        &quot;and&quot;            &quot;also&quot;           &quot;one&quot;           
##  [21] &quot;of&quot;             &quot;the&quot;            &quot;most&quot;           &quot;talented&quot;      
##  [25] &quot;actresses&quot;      &quot;meryl&quot;          &quot;streep&quot;         &quot;out&quot;           
##  [29] &quot;of&quot;             &quot;africa&quot;         &quot;she&quot;            &quot;is&quot;            
##  [33] &quot;wonderful&quot;      &quot;is&quot;             &quot;every&quot;          &quot;part&quot;          
##  [37] &quot;that&quot;           &quot;she&quot;            &quot;plays&quot;          &quot;the&quot;           
##  [41] &quot;?&quot;              &quot;graduate&quot;       &quot;is&quot;             &quot;the&quot;           
##  [45] &quot;pride&quot;          &quot;and&quot;            &quot;joy&quot;            &quot;of&quot;            
##  [49] &quot;the&quot;            &quot;american&quot;       &quot;cinema&quot;         &quot;br&quot;            
##  [53] &quot;br&quot;             &quot;?&quot;              &quot;health&quot;         &quot;is&quot;            
##  [57] &quot;?&quot;              &quot;and&quot;            &quot;her&quot;            &quot;husband&quot;       
##  [61] &quot;george&quot;         &quot;role&quot;           &quot;well&quot;           &quot;developed&quot;     
##  [65] &quot;by&quot;             &quot;brilliant&quot;      &quot;actor&quot;          &quot;and&quot;           
##  [69] &quot;also&quot;           &quot;oscar&quot;          &quot;winner&quot;         &quot;william&quot;       
##  [73] &quot;hurt&quot;           &quot;smoke&quot;          &quot;kiss&quot;           &quot;of&quot;            
##  [77] &quot;the&quot;            &quot;spider&quot;         &quot;woman&quot;          &quot;has&quot;           
##  [81] &quot;a&quot;              &quot;hard&quot;           &quot;time&quot;           &quot;with&quot;          
##  [85] &quot;the&quot;            &quot;?&quot;              &quot;health&quot;         &quot;of&quot;            
##  [89] &quot;his&quot;            &quot;one&quot;            &quot;true&quot;           &quot;thing&quot;         
##  [93] &quot;and&quot;            &quot;seeks&quot;          &quot;his&quot;            &quot;daughter&#39;s&quot;    
##  [97] &quot;help&quot;           &quot;the&quot;            &quot;poor&quot;           &quot;daughter&quot;      
## [101] &quot;ellen&quot;          &quot;?&quot;              &quot;?&quot;              &quot;jerry&quot;         
## [105] &quot;maguire&quot;        &quot;has&quot;            &quot;way&quot;            &quot;too&quot;           
## [109] &quot;much&quot;           &quot;expected&quot;       &quot;of&quot;             &quot;her&quot;           
## [113] &quot;no&quot;             &quot;breaks&quot;         &quot;the&quot;            &quot;story&quot;         
## [117] &quot;takes&quot;          &quot;a&quot;              &quot;very&quot;           &quot;realistic&quot;     
## [121] &quot;view&quot;           &quot;on&quot;             &quot;the&quot;            &quot;illness&quot;       
## [125] &quot;of&quot;             &quot;a&quot;              &quot;parent&quot;         &quot;in&quot;            
## [129] &quot;this&quot;           &quot;movie&quot;          &quot;the&quot;            &quot;only&quot;          
## [133] &quot;daughter&quot;       &quot;has&quot;            &quot;to&quot;             &quot;put&quot;           
## [137] &quot;her&quot;            &quot;life&quot;           &quot;on&quot;             &quot;hold&quot;          
## [141] &quot;to&quot;             &quot;care&quot;           &quot;for&quot;            &quot;the&quot;           
## [145] &quot;needs&quot;          &quot;of&quot;             &quot;others&quot;         &quot;there&quot;         
## [149] &quot;is&quot;             &quot;always&quot;         &quot;one&quot;            &quot;in&quot;            
## [153] &quot;every&quot;          &quot;family&quot;         &quot;who&quot;            &quot;faces&quot;         
## [157] &quot;that&quot;           &quot;kind&quot;           &quot;of&quot;             &quot;responsibility&quot;
## [161] &quot;ellen&quot;          &quot;is&quot;             &quot;angry&quot;          &quot;the&quot;           
## [165] &quot;beginning&quot;      &quot;of&quot;             &quot;the&quot;            &quot;movie&quot;         
## [169] &quot;but&quot;            &quot;as&quot;             &quot;time&quot;           &quot;passes&quot;        
## [173] &quot;she&quot;            &quot;ends&quot;           &quot;up&quot;             &quot;understanding&quot; 
## [177] &quot;her&quot;            &quot;?&quot;              &quot;life&quot;           &quot;time&quot;          
## [181] &quot;dedication&quot;     &quot;to&quot;             &quot;her&quot;            &quot;family&quot;        
## [185] &quot;she&quot;            &quot;even&quot;           &quot;asks&quot;           &quot;her&quot;           
## [189] &quot;mom&quot;            &quot;how&quot;            &quot;do&quot;             &quot;you&quot;           
## [193] &quot;do&quot;             &quot;his&quot;            &quot;every&quot;          &quot;day&quot;           
## [197] &quot;in&quot;             &quot;and&quot;            &quot;out&quot;            &quot;and&quot;           
## [201] &quot;nobody&quot;         &quot;notices&quot;        &quot;it&quot;             &quot;that&quot;          
## [205] &quot;is&quot;             &quot;what&quot;           &quot;women&quot;          &quot;do&quot;            
## [209] &quot;a&quot;              &quot;lot&quot;            &quot;of&quot;             &quot;what&quot;          
## [213] &quot;i&quot;              &quot;call&quot;           &quot;invisible&quot;      &quot;work&quot;          
## [217] &quot;moreover&quot;       &quot;we&quot;             &quot;clean&quot;          &quot;we&quot;            
## [221] &quot;fix&quot;            &quot;we&quot;             &quot;?&quot;              &quot;we&quot;            
## [225] &quot;stretch&quot;        &quot;we&quot;             &quot;celebrate&quot;      &quot;we&quot;            
## [229] &quot;are&quot;            &quot;the&quot;            &quot;best&quot;           &quot;friends&quot;       
## [233] &quot;we&quot;             &quot;are&quot;            &quot;the&quot;            &quot;mistress&quot;      
## [237] &quot;sensitive&quot;      &quot;some&quot;           &quot;of&quot;             &quot;us&quot;            
## [241] &quot;like&quot;           &quot;both&quot;           &quot;women&quot;          &quot;in&quot;            
## [245] &quot;this&quot;           &quot;movie&quot;          &quot;have&quot;           &quot;the&quot;           
## [249] &quot;perfect&quot;        &quot;education&quot;      &quot;are&quot;            &quot;the&quot;           
## [253] &quot;psychological&quot;  &quot;?&quot;              &quot;for&quot;            &quot;the&quot;           
## [257] &quot;entire&quot;         &quot;family&quot;         &quot;and&quot;            &quot;also&quot;          
## [261] &quot;do&quot;             &quot;all&quot;            &quot;that&quot;           &quot;invisible&quot;     
## [265] &quot;work&quot;           &quot;that&quot;           &quot;is&quot;             &quot;kate&quot;          
## [269] &quot;ellen&quot;          &quot;and&quot;            &quot;many&quot;           &quot;women&quot;         
## [273] &quot;in&quot;             &quot;our&quot;            &quot;society&quot;        &quot;many&quot;          
## [277] &quot;of&quot;             &quot;us&quot;             &quot;have&quot;           &quot;already&quot;       
## [281] &quot;gone&quot;           &quot;through&quot;        &quot;that&quot;           &quot;stage&quot;         
## [285] &quot;of&quot;             &quot;life&quot;           &quot;when&quot;           &quot;our&quot;           
## [289] &quot;parents&quot;        &quot;age&quot;            &quot;and&quot;            &quot;died&quot;          
## [293] &quot;i&quot;              &quot;have&quot;           &quot;been&quot;           &quot;there&quot;         
## [297] &quot;they&quot;           &quot;just&quot;           &quot;went&quot;           &quot;too&quot;           
## [301] &quot;young&quot;          &quot;i&quot;              &quot;have&quot;           &quot;given&quot;         
## [305] &quot;my&quot;             &quot;parents&quot;        &quot;my&quot;             &quot;thanks&quot;        
## [309] &quot;but&quot;            &quot;i&quot;              &quot;never&quot;          &quot;understood&quot;    
## [313] &quot;them&quot;           &quot;as&quot;             &quot;well&quot;           &quot;as&quot;            
## [317] &quot;when&quot;           &quot;i&quot;              &quot;had&quot;            &quot;to&quot;            
## [321] &quot;play&quot;           &quot;their&quot;          &quot;roles&quot;          &quot;and&quot;           
## [325] &quot;had&quot;            &quot;to&quot;             &quot;walk&quot;           &quot;in&quot;            
## [329] &quot;their&quot;          &quot;shoes&quot;          &quot;this&quot;           &quot;movie&quot;         
## [333] &quot;mirrors&quot;        &quot;the&quot;            &quot;reality&quot;        &quot;of&quot;            
## [337] &quot;life&quot;           &quot;perhaps&quot;        &quot;it&quot;             &quot;is&quot;            
## [341] &quot;sad&quot;            &quot;but&quot;            &quot;that&quot;           &quot;is&quot;            
## [345] &quot;how&quot;            &quot;life&quot;           &quot;is&quot;             &quot;at&quot;            
## [349] &quot;times&quot;          &quot;george&quot;         &quot;a&quot;              &quot;professor&quot;     
## [353] &quot;at&quot;             &quot;?&quot;              &quot;is&quot;             &quot;complicated&quot;   
## [357] &quot;person&quot;         &quot;who&quot;            &quot;appears&quot;        &quot;to&quot;            
## [361] &quot;think&quot;          &quot;that&quot;           &quot;his&quot;            &quot;work&quot;          
## [365] &quot;is&quot;             &quot;more&quot;           &quot;important&quot;      &quot;than&quot;          
## [369] &quot;everybody&quot;      &quot;else&quot;           &quot;and&quot;            &quot;has&quot;           
## [373] &quot;a&quot;              &quot;very&quot;           &quot;master&quot;         &quot;servant&quot;       
## [377] &quot;mentality&quot;      &quot;toward&quot;         &quot;the&quot;            &quot;women&quot;         
## [381] &quot;in&quot;             &quot;his&quot;            &quot;life&quot;           &quot;he&quot;            
## [385] &quot;is&quot;             &quot;not&quot;            &quot;strong&quot;         &quot;enough&quot;        
## [389] &quot;to&quot;             &quot;cope&quot;           &quot;if&quot;             &quot;you&quot;           
## [393] &quot;want&quot;           &quot;to&quot;             &quot;see&quot;            &quot;good&quot;          
## [397] &quot;acting&quot;         &quot;and&quot;            &quot;the&quot;            &quot;reality&quot;       
## [401] &quot;of&quot;             &quot;life&quot;           &quot;do&quot;             &quot;not&quot;           
## [405] &quot;miss&quot;           &quot;this&quot;           &quot;movie&quot;          &quot;favorite&quot;      
## [409] &quot;scenes&quot;         &quot;the&quot;            &quot;restaurant&quot;     &quot;coming&quot;        
## [413] &quot;to&quot;             &quot;kate&quot;           &quot;?&quot;              &quot;and&quot;           
## [417] &quot;all&quot;            &quot;the&quot;            &quot;making&quot;         &quot;of&quot;            
## [421] &quot;a&quot;              &quot;table&quot;          &quot;out&quot;            &quot;of&quot;            
## [425] &quot;broken&quot;         &quot;china&quot;          &quot;that&quot;           &quot;i&quot;             
## [429] &quot;so&quot;             &quot;symbolic&quot;       &quot;we&quot;             &quot;are&quot;           
## [433] &quot;all&quot;            &quot;broken&quot;         &quot;?&quot;              &quot;favorite&quot;      
## [437] &quot;quotes&quot;         &quot;george&quot;         &quot;it&quot;             &quot;is&quot;            
## [441] &quot;only&quot;           &quot;by&quot;             &quot;going&quot;          &quot;?&quot;             
## [445] &quot;that&quot;           &quot;you&quot;            &quot;realize&quot;        &quot;that&quot;          
## [449] &quot;you&quot;            &quot;are&quot;            &quot;really&quot;         &quot;going&quot;         
## [453] &quot;downhill&quot;       &quot;george&quot;         &quot;you&quot;            &quot;have&quot;          
## [457] &quot;a&quot;              &quot;?&quot;              &quot;education&quot;      &quot;but&quot;           
## [461] &quot;where&quot;          &quot;is&quot;             &quot;your&quot;           &quot;heart&quot;         
## [465] &quot;br&quot;             &quot;br&quot;</code></pre>
<p>Well, that’s one loooooong review…</p>
</div>
</div>

</div>
            <div class="footer">
                <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>


        <h2>Comments</h2>
        <div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://statslab.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

            </div>
        </div>
        
                
    </body>
</html>
