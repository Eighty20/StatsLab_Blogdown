<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 22 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predict house prices - deep learning, keras</title>
      <link>/posts/predict_house_prices_dnn/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/predict_house_prices_dnn/</guid>
      <description>OverviewNaive model (no time index)Load the dataScale the variablesDefine the modelMeasuring over-fit using k-folds crossvalidationGet resultsBenchmark vs Gradient boosting machinesTime series models using LSTM together with an inference networkRead in the dataRead in the dataProcess dataDesign inference modelDesign LSTM modelTest a LSTM modelEverything set… time to get started!Back test LSTM modelCombine LSTM and Inference networks into 1 deep neural network?</description>
    </item>
    
    <item>
      <title>Let&#39;s play with autoencoders (Keras, R)</title>
      <link>/posts/autoencoders_keras_r/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/autoencoders_keras_r/</guid>
      <description>What are autoencoders?How do we build them?Build itStep 1 - load and prepare the dataStep 2 - define the encoder and decoderStep 3 - compile and train the autoencoderStep 4 - Extract the weights of the encoderStep 5 - Load up the weights into an ecoder model and predictConclusionlibrary(ggplot2)library(keras)library(tidyverse)## ── Attaching packages ──────────────────────────────────────────────────── tidyverse 1.</description>
    </item>
    
    <item>
      <title>Serving a machine learning model via API</title>
      <link>/posts/testing_apis_in_r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/testing_apis_in_r/</guid>
      <description>About the Plumer packageIn oder to serve our API we will make use of the great Plumer package in R
To read more about this package go to:
https://www.rplumber.io/docs/
SetupLoad in some packages.
If you are going to host the api on a suse or redhat linux server make sure you have all the dependencies as well as the packages installed to follow through this example yourself.</description>
    </item>
    
    <item>
      <title>Blogging from a Python Jupyter Notebook</title>
      <link>/posts/blogging_from_jupyter/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/blogging_from_jupyter/</guid>
      <description>This is a follow on post to Stefan&amp;rsquo;s original to show how to generate a blog post from a Jupyter Notebook instead of an R markdown. This post itself started off life as a Jupyter Notebook which lives in the same content/posts folder as the other Rmd files used for the site. We&amp;rsquo;ll walk through how it became a blog post.
The process is a little more complicated than for the Rmd files (since that&amp;rsquo;s what Blogdown was built for but we can still get it to work relatively easily.</description>
    </item>
    
    <item>
      <title>How to add a blog to Blogdown</title>
      <link>/posts/how_to_add_a_blog/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/how_to_add_a_blog/</guid>
      <description>Pre-requisitesCreate from scratchClone or open the blog repository from githubCreate a post using “Addins” dropdownChange/check your date formatWrite a kickass postCompile your new work using BlogdownPush your post to the websiteCreate from existing RmdPre-requisitesInstall blogdown and Hugo in R-Console
https://bookdown.org/yihui/blogdown/installation.htmlCreate from scratchCreating it from scratch is probably the easiest since you can run and test your code as you type it up.</description>
    </item>
    
    <item>
      <title>Dealing with nested data</title>
      <link>/posts/using_e-sports_api/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using_e-sports_api/</guid>
      <description>Dealing with nested data can be really frustrating…
Especially if you want to keep your workspace nice and tidy with all your data in tables!
With no actual experience trying to get at these nested tibbles can seem almost impossible:
via GIPHY
--Downloading data from an api created by BlizzardTo illustrate how you would deal with nested data I found an api that let’s you download all kinds of data on the e-sport/game called Overwatch.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/posts/data-wrangling/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-wrangling/</guid>
      <description>Why data-wranglingIf you can wrangle data into the proper form you can do anything with it…
Data-wrangling is absolutely essential for every data science task where we need to work with collected data.
A recent article from the New York Times said “Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information.</description>
    </item>
    
    <item>
      <title>Data Science Workflow</title>
      <link>/posts/data-science-workflow/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-science-workflow/</guid>
      <description>Why it mattersWhen working with recurring clients and projects together with tight deadlines it is easy to cut corners or forget good practice.
Here are the top reasons why a data science workflow is vital:
Work is well documented
Folders and files logically are structured
Projects remain version controlled
Data output is tracked
Logical seperation of scripts make productizing easy
Good naming conventions make tracking of the project flow easy</description>
    </item>
    
    <item>
      <title>Benchmarking machine learning models in parallel</title>
      <link>/posts/benchmarking_in_parallel/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/benchmarking_in_parallel/</guid>
      <description>OverviewHaving just started playing with deeplearning models in R, I wanted to visually compare them to other more traditional ML workflows. Of course, deeplearning is generally used where other models fail, but with no need for feature selection and rapidly increasing power and ease of use they may just evolve into a general learning paradigm.
However, with tabular data and packages like caret the machine learning methods have become so streamlined that minimal user input is required at all.</description>
    </item>
    
    <item>
      <title>Design and Analysis of Experiments with R</title>
      <link>/posts/design_and_analysis_of_experiments_with_r/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/design_and_analysis_of_experiments_with_r/</guid>
      <description>IntroductionThis document will cover in depth the use of DoE experiments in R! We will focus our attention to measuring treatment effects, not variances.
Based on the work:
Lawson, John. Design and Analysis of Experiments with R. Chapman and Hall/CRC, 20141217. VitalBook file.
DefinitionsExperiment (also called a Run) is an action where the experimenter changes at least one of the variables being studied and then observes the effect of his or her actions(s).</description>
    </item>
    
    <item>
      <title>A Soft Introduction to Machine Learning</title>
      <link>/posts/intro-machine-learning/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/intro-machine-learning/</guid>
      <description>Important machine learning librariesFuture additionsPart 1 - Data PreprocessingPart 2 - RegressionSimple Linear RegressionMultiple linear regressionPolynomial RegressionSupport vector machine regressionRegression TreesRandom forest regressionA more robust application of machine learning regressions (random forest)Part 3 - ClusteringK-meansPart 4 - Dimensionality ReductionCreate PCAPart 5 - Reinforced LearningMulti-Armed Bandit ProblemUpper Confidence Bound (UCB) methodImprove results using UCBVisualize the model add selectionPart 6 - Parameter Grid Search, Cross-validation and BoostingGrid Search and Parameter TuningXGBoostImportant machine learning librariesI created this document to serve as an easy introduction to basic machine learning methods.</description>
    </item>
    
    <item>
      <title>About Hugo</title>
      <link>/about/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hugo is a static site engine written in Go.
It makes use of a variety of open source projects including:
 Cobra Viper J Walter Weatherman Cast  Learn more and contribute on GitHub.</description>
    </item>
    
  </channel>
</rss>