<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on </title>
    <link>/categories/r/</link>
    <description>Recent content in R on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 22 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predict house prices - deep learning, keras</title>
      <link>/posts/predict_house_prices_dnn/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/predict_house_prices_dnn/</guid>
      <description>OverviewNaive model (no time index)Load the dataScale the variablesDefine the modelMeasuring over-fit using k-folds crossvalidationGet resultsBenchmark vs Gradient boosting machinesTime series models using LSTM together with an inference networkRead in the dataRead in the dataProcess dataDesign inference modelDesign LSTM modelTest a LSTM modelEverything set… time to get started!Back test LSTM modelCombine LSTM and Inference networks into 1 deep neural network?</description>
    </item>
    
    <item>
      <title>Let&#39;s play with autoencoders (Keras, R)</title>
      <link>/posts/autoencoders_keras_r/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/autoencoders_keras_r/</guid>
      <description>What are autoencoders?How do we build them?Build itStep 1 - load and prepare the dataStep 2 - define the encoder and decoderStep 3 - compile and train the autoencoderStep 4 - Extract the weights of the encoderStep 5 - Load up the weights into an ecoder model and predictConclusionlibrary(ggplot2)library(keras)library(tidyverse)## ── Attaching packages ──────────────────────────────────────────────────── tidyverse 1.</description>
    </item>
    
    <item>
      <title>Serving a machine learning model via API</title>
      <link>/posts/testing_apis_in_r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/testing_apis_in_r/</guid>
      <description>About the Plumer packageIn oder to serve our API we will make use of the great Plumer package in R
To read more about this package go to:
https://www.rplumber.io/docs/
SetupLoad in some packages.
If you are going to host the api on a suse or redhat linux server make sure you have all the dependencies as well as the packages installed to follow through this example yourself.</description>
    </item>
    
    <item>
      <title>How to add a blog to Blogdown</title>
      <link>/posts/how_to_add_a_blog/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/how_to_add_a_blog/</guid>
      <description>Pre-requisitesCreate from scratchClone or open the blog repository from githubCreate a post using “Addins” dropdownChange/check your date formatWrite a kickass postCompile your new work using BlogdownPush your post to the websiteCreate from existing RmdPre-requisitesInstall blogdown and Hugo in R-Console
https://bookdown.org/yihui/blogdown/installation.htmlCreate from scratchCreating it from scratch is probably the easiest since you can run and test your code as you type it up.</description>
    </item>
    
    <item>
      <title>Dealing with nested data</title>
      <link>/posts/using_e-sports_api/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using_e-sports_api/</guid>
      <description>Dealing with nested data can be really frustrating…
Especially if you want to keep your workspace nice and tidy with all your data in tables!
With no actual experience trying to get at these nested tibbles can seem almost impossible:
via GIPHY
--Downloading data from an api created by BlizzardTo illustrate how you would deal with nested data I found an api that let’s you download all kinds of data on the e-sport/game called Overwatch.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/posts/data-wrangling/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-wrangling/</guid>
      <description>Why data-wranglingIf you can wrangle data into the proper form you can do anything with it…
Data-wrangling is absolutely essential for every data science task where we need to work with collected data.
A recent article from the New York Times said “Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information.</description>
    </item>
    
    <item>
      <title>Benchmarking machine learning models in parallel</title>
      <link>/posts/benchmarking_in_parallel/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/benchmarking_in_parallel/</guid>
      <description>OverviewHaving just started playing with deeplearning models in R, I wanted to visually compare them to other more traditional ML workflows. Of course, deeplearning is generally used where other models fail, but with no need for feature selection and rapidly increasing power and ease of use they may just evolve into a general learning paradigm.
However, with tabular data and packages like caret the machine learning methods have become so streamlined that minimal user input is required at all.</description>
    </item>
    
    <item>
      <title>A Soft Introduction to Machine Learning</title>
      <link>/posts/intro-machine-learning/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/intro-machine-learning/</guid>
      <description>Important machine learning librariesFuture additionsPart 1 - Data PreprocessingPart 2 - RegressionSimple Linear RegressionMultiple linear regressionPolynomial RegressionSupport vector machine regressionRegression TreesRandom forest regressionA more robust application of machine learning regressions (random forest)Part 3 - ClusteringK-meansPart 4 - Dimensionality ReductionCreate PCAPart 5 - Reinforced LearningMulti-Armed Bandit ProblemUpper Confidence Bound (UCB) methodImprove results using UCBVisualize the model add selectionPart 6 - Parameter Grid Search, Cross-validation and BoostingGrid Search and Parameter TuningXGBoostImportant machine learning librariesI created this document to serve as an easy introduction to basic machine learning methods.</description>
    </item>
    
  </channel>
</rss>