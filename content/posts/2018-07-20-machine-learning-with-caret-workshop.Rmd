---
title: Machine learning artistry
author: Stefan
date: '2018-07-20'
slug: machine-learning-with-caret-workshop
categories:
  - R
tags:
  - caret
  - benchmarking
  - Cross-validation
  - machine_learning
  - Workflow
  - Regression
  - recipes
  - workshop
---

## Methodical measurement of class imbalance (recipes and rsample)

## Overview

This blog serves as the reference material for our workshop on using caret for machine learning workflow in R.

More importantly, this blog is maticulously designed so as to be an all encompassing post about using caret as your first angle of attack for supervised learning.

Because of this I have designed each section to be wholly independent and address a specific use case. I also highlight some intuition throughout the post but most intuition will be caught, and not taught, during the workshop.

## libraries

These are the basic libraries you will need. If you are not familiar with any of these I suggest you at least read through their documentation to understand the role of each:

```{r}
library(tidyverse)
library(caret)
library(rsample)
library(recipes)
library(yardstick)
```

```{r}
dataset <- iris
```

## recipes

The recipes package is an attempt to standardize all data treatment prior to model training. Much like the rsample and caret package this library wants to be a rich and compatible way to build your own machine learning workflows.  

So why a recipe? If all I want to do is transform the data a bit, I can do that myself?  

This is true. But as you would have found it is easy to run into some simple issues with even the most basic of transformations.  

For example; I say I want to centre and scale the data. I do so and then I split the data into training and testing sets. I realize that transforming the training data using features from the test data like outliers has leaked signal that I did not intend to leak. I correct this; I split the data first... I realize that applying the function to both sets will scale the 2 sets differently.  

You can see how things can get tricky. More than this the package also has exceptional functions for just about anything you could possibly want to do with the data before your training.  

For a list of the possible transformations please look at <https://tidymodels.github.io/recipes/reference/index.html>  

```{r}
rec <-
  recipe(Species~.,data=dataset) %>%
  step_corr(all_numeric(),threshold = 0.9) %>%
  step_nzv(all_numeric()) %>%
  step_naomit(everything()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())
```


## bootstrapping

```{r}
bt_samples <- bootstraps(dataset,times = 5)
bt_samples
```

```{r}
# bt_samples$recipes <- map(bt_samples$splits, prepper, recipe = rec, retain = TRUE, verbose = FALSE)

bt_samples <- 
  bt_samples %>% 
  mutate(prepared_recipe = splits %>% map(~.x %>% prepper(recipe = rec, retain = TRUE, verbose = FALSE)))

bt_samples
```


```{r}
bt_samples <- 
  bt_samples %>% 
  mutate(models = prepared_recipe %>% map(~train(form = Species~.,data = juice(.x),method = "rf")))

bt_samples
```

```{r}
bt_samples <- 
  bt_samples %>% 
  mutate(validated_predictions = pmap(list(splits,prepared_recipe,models),~bake(..2,assessment(..1)) %>% predict(object =..3))) %>% 
  mutate(validated_predictions_prob = pmap(list(splits,prepared_recipe,models),~bake(..2,assessment(..1)) %>% predict(object =..3,type="prob")))

bt_samples
```

#### Calculate performance metrics using our bootstrapping

```{r}

bt_samples <- 
  bt_samples %>%
  mutate(
    J_index_ = pmap_dbl(list(splits,validated_predictions),~tibble(Class = assessment(..1) %>% pull(Species),estimate = ..2) %>%  j_index(truth = "Class", estimate = "estimate")))

```


```{r}
data_partition <- dataset %>% initial_split(prop = 0.8)

train <- data_partition %>% training()
test <- data_partition %>% testing()

prepped_data %>% select_if(is.numeric) %>% cor() %>% corrplot()

rec_obj <-
  recipe(outcome~.,data=prepped_data) %>%
  step_corr(all_numeric(),threshold = 0.9) %>%
  step_nzv(all_numeric()) %>%
  # step_lag(outcome,lag = c(1,2)) %>%
  step_naomit(everything()) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

prepped_rec <-
  rec_obj %>%
  prep(training = train)

train <-
  prepped_rec %>%
  bake(newdata = train)

test <-
  prepped_rec %>%
  bake(newdata = test)

train %>% head
```

## Benchmark some models

set crossvalidation parameters

```{r}
trControl <- trainControl(method = "cv",number = 4)
```

build model framework

```{r}
train_data <-
    tibble(model = c("rf","gbm"),data = list(iris))
```

train models on 16 cores

```{r}
set.seed(8020)
cl = makeCluster(16)
doParallel::registerDoParallel(cl)

system.time({
model_frame <- train_data %>%
    mutate(caret_models = map2(.x = model,.y=data,~train(form = outcome~., data = .y,method = .x,trControl = trControl)))
})

stopCluster(cl)
```

Confusion matrices

```{r}
model_frame %<>%
    mutate(confusion_matrix = map(caret_models,~confusionMatrix(.x)))

model_frame$confusion_matrix
```

On holdout data

```{r}

confusion_mat_test_set <-
table(
model_frame$caret_models[[2]] %>% predict(newdata = test),
  test$outcome
)

confusion_mat_test_set
cat("accuracy:", confusion_mat_test_set %>% diag %>% sum/sum(confusion_mat_test_set),"  , \n")

```

Variable importance

```{r}
varImp(model_frame$caret_models[[2]]) %>%
```
