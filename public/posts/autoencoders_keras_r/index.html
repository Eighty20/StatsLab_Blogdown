<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Enable responsiveness on mobile devices --> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    
    	
    <meta name="generator" content="Hugo 0.37.1" />
    
    <title>Let&#39;s play with autoencoders (Keras, R) &middot; </title>
    <meta content="Let&#39;s play with autoencoders (Keras, R) - " property="og:title">
    <meta content=" - " property="og:description">    
    <!-- CSS --> 
    <link rel="stylesheet" href="../../css/print.css" media="print">
    <link rel="stylesheet" href="../../css/poole.css">
    <link rel="stylesheet" href="../../css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,500">
    
    <script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
    <!-- highlight.js--> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <!-- Customised CSS -->
    <link rel="stylesheet" href="../../css/custom.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="../../favicon.png">
    

	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="../../"><h1 class="brand">StatsLab</h1></a>
			 <img src="../../img/stats.png" alt="Author Image" class="img-circle headshot center"> 
			<p class="lead">
				 Internal initiative at <br> <a href="http://www.eighty20.co.za/"> Eighty20</a> <br> We love R and Python 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="../../posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="../../about_me/"> <span>About</span></a>
				</li>
				<li>
					<a href="../../tags/"> <span>Search by Tag</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	
	
	
	
	
	
	
</section>

        </p>
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="post">
  <h1>Let&#39;s play with autoencoders (Keras, R)</h1>
  
  <div class="col-sm-12 col-md-12">
    <span class="text-left post-date meta">
            
       <i class="fa fa-calendar" aria-hidden="true"></i> Apr 4, 2018 
      
      
        
        
            in
            
            
                <a class="meta" href="../../categories/r">R</a>
                
            
        
      
      
      
        
        
            <br/>
             <i class="fa fa-tags" aria-hidden="true"></i>
            
            <a class="meta" href="../../tags/deep_learning">deep_learning</a> 
        
            <a class="meta" href="../../tags/dimensionality_reduction">dimensionality_reduction</a> 
        
            <a class="meta" href="../../tags/keras">keras</a> 
        
            <a class="meta" href="../../tags/transfer_learning">transfer_learning</a>
        
      
      
      </span>  
  </div>    
  
  <div id="TOC">
<ul>
<li><a href="#what-are-autoencoders">What are autoencoders?</a></li>
<li><a href="#how-do-we-build-them">How do we build them?</a></li>
<li><a href="#build-it">Build it</a><ul>
<li><a href="#step-1---load-and-prepare-the-data">Step 1 - load and prepare the data</a></li>
<li><a href="#step-2---define-the-encoder-and-decoder">Step 2 - define the encoder and decoder</a></li>
<li><a href="#step-3---compile-and-train-the-autoencoder">Step 3 - compile and train the autoencoder</a></li>
<li><a href="#step-4---extract-the-weights-of-the-encoder">Step 4 - Extract the weights of the encoder</a></li>
<li><a href="#step-5---load-up-the-weights-into-an-ecoder-model-and-predict">Step 5 - Load up the weights into an ecoder model and predict</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul>
</div>

<pre class="r"><code>library(ggplot2)
library(keras)
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ tibble  1.4.2     ✔ purrr   0.2.4
## ✔ tidyr   0.8.0     ✔ dplyr   0.7.4
## ✔ readr   1.1.1     ✔ stringr 1.3.0
## ✔ tibble  1.4.2     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<div id="what-are-autoencoders" class="section level2">
<h2>What are autoencoders?</h2>
<p>The main purpose of an encoder is to map high dimensional data into lower dimensional data by minimizing the loss of decoding accuracy.</p>
<p><em>How is that different from dimensionality reduction?</em></p>
<p>Well.. it is and it isn’t… In it’s simplest case the neural network can use basicly linear activation functions to approximate a PCA. The important thing is that it is an approximation.<br />
Autoencoders are not unsupervised learning algorithms, instead they are self-learning algorithms since the network will try to replicate itself from lower dimensional space. In this sense the net will not really extract those features that are usefull to understand the data as much as it will extract the most important features to reproduce the data.</p>
<p>We can however perform dimensionality reduction if we redefine the loss function however:<br />
<a href="https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb">https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb</a></p>
<p>In that great python notebook the author <em>kylemcdonald</em> defines a t-sne loss function in python and uses it to perform autoencoding via t-sne.</p>
</div>
<div id="how-do-we-build-them" class="section level2">
<h2>How do we build them?</h2>
<p>We can build one quite simply by defining the encoding layers and the decoding layers;</p>
<div class="figure">
<img src="../../Pictures/Autoencoders/what_are_autoencoders.png" />

</div>
<p>Initially people did this by defining a simple symmetrical neural network like this:</p>
<div class="figure">
<img src="../../Pictures/Autoencoders/autoencoder_net.png" />

</div>
<p>The initial half was responsible for encoding into lower dimensional space and the decoder was responible for mapping it back again to validate information loss.<br />
In practice the network does not need to be symmetrical at all and can be more complicated such as having CNN layers for image encoding.</p>
</div>
<div id="build-it" class="section level2">
<h2>Build it</h2>
<p>OK, so let’s show how the autoencoder is built by defining the encoder and decoder</p>
<div id="step-1---load-and-prepare-the-data" class="section level3">
<h3>Step 1 - load and prepare the data</h3>
<p>For the initial example we will use the iris dataset as our hello world showcase</p>
<div id="split-test-train" class="section level4">
<h4>Split test train</h4>
<pre class="r"><code>split_ind &lt;- iris$Species %&gt;% caret::createDataPartition(p = 0.8,list = FALSE)

train &lt;- iris[split_ind,]
test &lt;- iris[-split_ind,]</code></pre>
</div>
<div id="pre-process" class="section level4">
<h4>Pre-process</h4>
<p>Note, normally you would need to perform one-hot encoding for the classes but since we are not going to train the model to classify this isn’t really needed…</p>
<pre class="r"><code>train_X &lt;- train[,1:4] %&gt;% as.matrix()

train_y &lt;- train[,5] %&gt;% 
  keras::to_categorical()

test_X &lt;- test[,1:4] %&gt;% as.matrix()</code></pre>
</div>
</div>
<div id="step-2---define-the-encoder-and-decoder" class="section level3">
<h3>Step 2 - define the encoder and decoder</h3>
<p>It’s the encoder’s job to embed the data into lower dimensional space. So logically it should map from the initial input dimensions to the specified number of perceptrons in the output layer as new dimensions:</p>
<p>Notice that we define the different parts seperately because we are going to use the keras functional api instead in order to keep the individual models <code>encoder</code> and <code>decoder</code></p>
<pre class="r"><code>input_layer &lt;- 
  layer_input(shape = c(4)) 

encoder &lt;- 
  input_layer %&gt;% 
  layer_dense(units = 20, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;% 
  layer_dense(units = 10, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(rate = 0.1) %&gt;%
  layer_dense(units = 2) # 2 dimensions for the output layer

decoder &lt;- 
  encoder %&gt;% 
  layer_dense(units = 20, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(rate = 0.2) %&gt;% 
  layer_dense(units = 10, activation = &quot;relu&quot;) %&gt;%
  layer_dropout(rate = 0.1) %&gt;%
  layer_dense(units = 4) # 4 dimensions for the original 4 variables</code></pre>
</div>
<div id="step-3---compile-and-train-the-autoencoder" class="section level3">
<h3>Step 3 - compile and train the autoencoder</h3>
<p>To train the encoder we need to capture the initial input as our goal and back-propogate to best represent it<br />
We treat this as a basic regression so we will use some arbitrary regression loss function (nothing fancy like t-sne)</p>
<pre class="r"><code>autoencoder_model &lt;- keras_model(inputs = input_layer, outputs = decoder)

autoencoder_model %&gt;% compile(
  loss=&#39;mean_squared_error&#39;,
  optimizer=&#39;rmsprop&#39;,
  metrics = c(&#39;accuracy&#39;)
)

summary(autoencoder_model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## input_1 (InputLayer)             (None, 4)                     0           
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 20)                    100         
## ___________________________________________________________________________
## dropout_1 (Dropout)              (None, 20)                    0           
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 10)                    210         
## ___________________________________________________________________________
## dropout_2 (Dropout)              (None, 10)                    0           
## ___________________________________________________________________________
## dense_3 (Dense)                  (None, 2)                     22          
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 20)                    60          
## ___________________________________________________________________________
## dropout_3 (Dropout)              (None, 20)                    0           
## ___________________________________________________________________________
## dense_5 (Dense)                  (None, 10)                    210         
## ___________________________________________________________________________
## dropout_4 (Dropout)              (None, 10)                    0           
## ___________________________________________________________________________
## dense_6 (Dense)                  (None, 4)                     44          
## ===========================================================================
## Total params: 646
## Trainable params: 646
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p>Now we train onto itself:</p>
<pre class="r"><code>history &lt;-
  autoencoder_model %&gt;%
  keras::fit(train_X,
             train_X,
             epochs=200,
             shuffle=TRUE,
             validation_data= list(test_X, test_X)
             )</code></pre>
<p>The training seems to have gone pretty well:</p>
<pre class="r"><code>plot(history)</code></pre>
<p><img src="../../posts/Building_an_autoencoder_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>So now that we believe we have trained our encoder to embed the data into lower dimensional space, let’s actually look at this data…</p>
<div id="visualize-the-embedding" class="section level4">
<h4>Visualize the embedding</h4>
<p>First we can use the complete model to visualize the reproduced points vs the actual points</p>
<pre class="r"><code>reconstructed_points &lt;- 
  autoencoder_model %&gt;% 
  keras::predict_on_batch(x = train_X)

Viz_data &lt;- 
  dplyr::bind_rows(
  reconstructed_points %&gt;% 
    tibble::as_tibble() %&gt;% 
    setNames(names(train_X %&gt;% tibble::as_tibble())) %&gt;% 
    dplyr::mutate(data_origin = &quot;reconstructed&quot;),
  train_X %&gt;% 
    tibble::as_tibble() %&gt;% 
    dplyr::mutate(data_origin = &quot;original&quot;)
  )

Viz_data %&gt;%
  ggplot(aes(Petal.Length,Sepal.Width, color = data_origin))+
  geom_point()</code></pre>
<p><img src="../../posts/Building_an_autoencoder_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>OK.. so this isn’t quite as sexy as I wanted… The lossy reconstruction is effectively just linear components. I expected the embedding to use linear compositions since we were activating the perceptrons using a regularized linear activation function (relu); the decoder seems to map these back linearly aswel when it returns to the higher dimensions which makes perfect sense…</p>
<p>What we can do is try to use non-linear activation functions… IF we wanted to reconstruct the data better, or use a more advanced loss function…</p>
</div>
</div>
<div id="step-4---extract-the-weights-of-the-encoder" class="section level3">
<h3>Step 4 - Extract the weights of the encoder</h3>
<p>Once we have decided on the autoencoder to use we can have a closer look at the encoder part only. There are various ways to do this but what I will do is extract the weights from the autoencoder and use them to define the encoder.</p>
<pre class="r"><code>autoencoder_weights &lt;- 
  autoencoder_model %&gt;%
  keras::get_weights()

autoencoder_weights</code></pre>
<pre><code>## [[1]]
##             [,1]       [,2]       [,3]       [,4]         [,5]       [,6]
## [1,] -0.19810045 -0.2890594 -0.4788326  0.3656170 -0.049993277 -0.3417195
## [2,] -0.13355756 -0.4421676  0.3013584 -0.4572745 -0.452414274 -0.1832947
## [3,]  0.05859601  0.4331032  0.4251073 -0.5050862 -0.348123550  0.1731703
## [4,] -0.21471357 -0.3656846 -0.2174333  0.2001393 -0.005069375  0.4370953
##            [,7]       [,8]        [,9]       [,10]      [,11]       [,12]
## [1,] -0.1274408  0.3313632  0.28551307 -0.20593393 -0.4492089 -0.27636841
## [2,]  0.1513306 -0.4916806 -0.03809054 -0.24016953  0.3094139  0.28224009
## [3,]  0.4905727  0.1920096 -0.24149956  0.26611352 -0.1585885  0.17703216
## [4,]  0.2369394  0.1479883  0.30163655  0.01633132 -0.3817316 -0.08643751
##            [,13]       [,14]      [,15]      [,16]      [,17]       [,18]
## [1,] -0.09903972  0.04008921  0.1687387 -0.4501876  0.3014244  0.08929887
## [2,] -0.17930681  0.44073483  0.2108887  0.3140634 -0.4458759 -0.09118154
## [3,]  0.58526063 -0.10169356 -0.2809908  0.1138010 -0.1019785  0.07665879
## [4,]  0.17452545 -0.58561099 -0.2378064  0.3785278  0.1946799 -0.26796687
##            [,19]      [,20]
## [1,]  0.13314283  0.3646372
## [2,] -0.42348254 -0.2315432
## [3,] -0.09321356  0.1183737
## [4,]  0.05795705  0.5200057
## 
## [[2]]
##  [1]  0.000000000  0.000000000 -0.003161951 -0.019725051  0.000000000
##  [6]  0.000000000 -0.180263743 -0.217021078 -0.098586477  0.000000000
## [11]  0.000000000 -0.039309889  0.020278711 -0.058002576  0.101324894
## [16]  0.000000000 -0.110244967 -0.082611360  0.000000000 -0.177449659
## 
## [[3]]
##              [,1]        [,2]        [,3]        [,4]        [,5]
##  [1,] -0.03894004 -0.03375557  0.18949765 -0.17067352 -0.43869475
##  [2,]  0.36783552 -0.25589052 -0.01764798  0.39698964  0.39313889
##  [3,]  0.18010288 -0.12951195 -0.11280555 -0.19076189  0.22096658
##  [4,] -0.38558343  0.23894741  0.12296630  0.15591188  0.08757484
##  [5,] -0.11243504  0.14250004 -0.20125094  0.21583134  0.17435014
##  [6,]  0.14326710 -0.21517880 -0.19693011 -0.08930275 -0.33504650
##  [7,] -0.10795767  0.11485808  0.18160379 -0.06430449 -0.10928646
##  [8,] -0.26547694  0.13660350  0.03848766  0.23132832 -0.06420188
##  [9,] -0.42181888 -0.12012659 -0.39370909  0.22361231  0.19659093
## [10,]  0.27169508  0.35066426  0.06871474 -0.24471964 -0.11188966
## [11,] -0.22306855  0.08765697  0.18052608  0.04330876  0.12052238
## [12,]  0.09297520 -0.29725099 -0.40671048 -0.01630458  0.26985162
## [13,] -0.44253582  0.49833247 -0.57835674  0.11484881 -0.39323771
## [14,] -0.51721698  0.17936355 -0.05137468 -0.41995195  0.42601341
## [15,]  0.25574234  0.13364848 -0.28598481 -0.20739955  0.50999117
## [16,]  0.40108347 -0.17217788 -0.18420377 -0.18087050 -0.14699978
## [17,]  0.20446181  0.34514898 -0.35951123  0.12897871 -0.30272102
## [18,] -0.49805468 -0.23209736 -0.12987073 -0.37494433 -0.26267153
## [19,] -0.34964705 -0.04805085 -0.19389239 -0.02458644  0.11011523
## [20,]  0.16908973 -0.15200214 -0.22241604 -0.42969307 -0.05286345
##               [,6]         [,7]        [,8]        [,9]       [,10]
##  [1,] -0.240142375  0.023140937 -0.20664227 -0.26783442 -0.44110852
##  [2,]  0.074159145  0.007384896  0.31950134 -0.43976739  0.41165566
##  [3,]  0.183514863 -0.110021919  0.37971011  0.30397260  0.13945091
##  [4,]  0.140333772 -0.358145744 -0.22441660  0.31381297  0.40524703
##  [5,]  0.398274541 -0.342501968  0.04684538 -0.11309290  0.30081648
##  [6,]  0.286519349 -0.238759667  0.17305923 -0.26082072  0.07186437
##  [7,] -0.092262588 -0.586024940 -0.22478434  0.27513564  0.47488835
##  [8,] -0.063490368  0.387344867  0.11991206  0.29121178 -0.05344317
##  [9,]  0.356103659  0.232027933 -0.16445430 -0.19064815 -0.18762636
## [10,] -0.247070685  0.033974051  0.38732278 -0.19709888 -0.18075097
## [11,] -0.149082363  0.017083406 -0.34205201 -0.28469437  0.30166316
## [12,] -0.146611214 -0.465063751  0.34985748 -0.31758896 -0.36653835
## [13,]  0.071415648 -0.067518100 -0.03439792 -0.28210276  0.08541378
## [14,] -0.316790551 -0.309899926 -0.20213093 -0.16570771  0.05380922
## [15,] -0.229673147 -0.314509600  0.01330821  0.03166856 -0.81230086
## [16,]  0.273134410  0.407705009 -0.44666523  0.36074919 -0.32692143
## [17,]  0.221754223  0.380223185 -0.25319594  0.26170602  0.17852189
## [18,] -0.105808079 -0.107351020  0.01348185 -0.20626256  0.45991096
## [19,]  0.004207075 -0.382707536 -0.33793035 -0.29532713  0.21018100
## [20,]  0.291756660  0.152211025 -0.21936661 -0.11031637  0.09880430
## 
## [[4]]
##  [1] -0.164989755  0.062442530 -0.252930522 -0.170619696  0.190151244
##  [6]  0.044753630 -0.049725801 -0.129439473  0.102696843  0.006308007
## 
## [[5]]
##               [,1]       [,2]
##  [1,] -0.099992625  0.2151853
##  [2,] -0.169366151 -0.3270152
##  [3,]  0.333940953  0.4739485
##  [4,]  0.357096016 -0.3882841
##  [5,] -0.399842054  0.7182775
##  [6,] -0.282579958 -0.3471902
##  [7,] -0.065623529 -0.3419088
##  [8,]  0.361520261  0.4591236
##  [9,] -0.339793622 -0.4714327
## [10,] -0.009791862 -0.2750720
## 
## [[6]]
## [1] -0.3258936 -0.2373049
## 
## [[7]]
##            [,1]       [,2]        [,3]      [,4]       [,5]       [,6]
## [1,] -0.1888530 -0.1206196 -0.08552689 0.2234489  0.2553632 0.28983513
## [2,] -0.2490205  0.2149596  0.04815696 0.5015796 -0.2703362 0.07381693
##            [,7]        [,8]        [,9]      [,10]     [,11]     [,12]
## [1,] -0.2698302 -0.05949345 -0.13880281 -0.3436176 0.2662680 0.1273860
## [2,] -0.2563400 -0.03058871 -0.02180988  0.3003343 0.3612468 0.3886905
##           [,13]     [,14]     [,15]       [,16]      [,17]      [,18]
## [1,] -0.3187206 0.1264165 0.3020318  0.09145548  0.1130742 -0.3761013
## [2,] -0.4473160 0.1822689 0.1883731 -0.14060608 -0.5188102  0.3334480
##            [,19]    [,20]
## [1,] -0.46610531 0.160027
## [2,]  0.01638205 0.222523
## 
## [[8]]
##  [1]  0.1845889837  0.2537658513  0.2065257579  0.0213488825  0.3283543885
##  [6] -0.0052222954  0.2338465601  0.2763383687  0.2175585628  0.2435832620
## [11] -0.0852066800 -0.0768314824  0.1543265730 -0.0670906082 -0.0345370695
## [16]  0.2570781410  0.2145044804  0.2253880203  0.2509106994  0.0001498339
## 
## [[9]]
##               [,1]         [,2]       [,3]         [,4]          [,5]
##  [1,] -0.302943170 -0.247839034  0.2887530  0.283746362  0.0481099896
##  [2,] -0.004463926 -0.198767290 -0.0362801  0.203669026 -0.1347922981
##  [3,]  0.359333068 -0.059126936 -0.2487426  0.521335781  0.1387051195
##  [4,]  0.240750283 -0.070055149  0.3714221 -0.364791602  0.4730302989
##  [5,] -0.235111162  0.014540625  0.4735353  0.029323546  0.4251201451
##  [6,] -0.148440421  0.308380634  0.2021237  0.468928546 -0.2170918882
##  [7,] -0.122121841 -0.019047543  0.1990578  0.246525049  0.4223416746
##  [8,] -0.145921990  0.334002852  0.3724014  0.444794893  0.5771111846
##  [9,] -0.291935772 -0.339749515 -0.1005611  0.552151978  0.1530620456
## [10,]  0.270987064 -0.402820915 -0.1042266 -0.225688472  0.2883968651
## [11,]  0.384994864  0.231372759  0.2329728 -0.012630370 -0.3365521729
## [12,] -0.475155234 -0.096955292 -0.4535032  0.009265635 -0.2319095135
## [13,] -0.038118348 -0.006507069  0.2595372 -0.212711453 -0.2457305342
## [14,] -0.130516604 -0.071218677  0.4298320  0.448291093 -0.3007882535
## [15,]  0.163858771 -0.069803312  0.2571833 -0.054933388 -0.0001836247
## [16,]  0.032319620 -0.467911869  0.2152630  0.237650052  0.4645350575
## [17,] -0.263895392  0.018485295  0.3593893  0.429615051  0.2455509305
## [18,]  0.230768234 -0.276973963 -0.2125749 -0.087341420  0.0429741405
## [19,]  0.073306590 -0.357077926 -0.2492246  0.071700208  0.1078038141
## [20,] -0.444687396 -0.213399902 -0.2123722 -0.269468576 -0.3457906246
##               [,6]        [,7]          [,8]        [,9]       [,10]
##  [1,]  0.178535670  0.17351045  0.1842543334  0.23075116  0.21882565
##  [2,]  0.647602916  0.02840295 -0.4337461591  0.29598966 -0.43013868
##  [3,]  0.532281816 -0.12885870 -0.1261554807  0.21171097 -0.09693190
##  [4,] -0.002721783  0.40018004  0.4060510099  0.49034721 -0.28964439
##  [5,] -0.052873258 -0.16862758  0.0535335690  0.54748756  0.56110889
##  [6,]  0.182305381  0.32027712  0.3708575666  0.33102712  0.37820217
##  [7,]  0.024390927 -0.50618172 -0.3237283528  0.06229535 -0.11348525
##  [8,]  0.173010752 -0.49907807 -0.3034935594  0.36931071  0.17000635
##  [9,]  0.438951254 -0.12622036 -0.3120369911 -0.11023337 -0.01551330
## [10,]  0.771064878  0.20872648  0.2456808984  0.13069907 -0.24047209
## [11,] -0.288431764 -0.09232726 -0.4041654766 -0.01239598  0.17245021
## [12,] -0.227437645 -0.09529109 -0.2081450671 -0.11221645 -0.21999294
## [13,]  0.057080537  0.06732071 -0.0202142652  0.42116067  0.44323167
## [14,] -0.215463534  0.11797385  0.0001696452 -0.23104015 -0.12428790
## [15,] -0.279793888  0.39239982  0.3036078513  0.21698692 -0.09208547
## [16,]  0.215835020 -0.11659818  0.2775081694  0.60408813 -0.05070640
## [17,] -0.133376762 -0.20384206 -0.4573314190 -0.18622299  0.30999789
## [18,]  0.530857027 -0.19506523 -0.2385864854  0.16266389 -0.21150532
## [19,]  0.010624185 -0.21198301 -0.2041848898  0.41898030  0.40622199
## [20,] -0.105845064 -0.21533197  0.1692898422 -0.10254069  0.11832178
## 
## [[10]]
##  [1] -0.1146519 -0.1473223  0.1587084  0.4174635  0.5583722  0.5481619
##  [7] -0.2333650 -0.1770816  0.4947782  0.4964104
## 
## [[11]]
##             [,1]        [,2]        [,3]        [,4]
##  [1,] -0.2165412 -0.29971039 -0.29913428 -0.74308932
##  [2,] -0.3699481 -0.11158245 -0.06216138  0.36666143
##  [3,]  0.1560502  0.08610131  0.59666258  0.49059626
##  [4,]  0.2414130  0.31613943  0.72625095  0.06958404
##  [5,]  0.9473153  0.18663064  0.11797066  0.16421179
##  [6,]  0.8052710  0.54204136 -0.13817216  0.04169955
##  [7,] -0.2085233  0.02315826 -0.23448357 -0.40083387
##  [8,] -0.2649664 -0.12844050  0.14209148 -0.17163362
##  [9,]  0.3040004  0.40178865  0.40049052  0.12716326
## [10,]  0.7680590 -0.09262366  0.59136415  0.12801459
## 
## [[12]]
## [1]  0.67448521  0.65254539  0.50650543 -0.09139419</code></pre>
<p>We can see that our encoder stops at the 6th layer… Since it produces the 2 dimensional output there.</p>
<p>We can also inspect the types of these variables:</p>
<pre class="r"><code>autoencoder_weights %&gt;% purrr::map_chr(class)</code></pre>
<pre><code>##  [1] &quot;matrix&quot; &quot;array&quot;  &quot;matrix&quot; &quot;array&quot;  &quot;matrix&quot; &quot;array&quot;  &quot;matrix&quot;
##  [8] &quot;array&quot;  &quot;matrix&quot; &quot;array&quot;  &quot;matrix&quot; &quot;array&quot;</code></pre>
<div id="save-the-weights" class="section level4">
<h4>Save the weights</h4>
<pre class="r"><code>  keras::save_model_weights_hdf5(object = autoencoder_model,filepath = &#39;../../static/data/autoencoder_weights.hdf5&#39;,overwrite = TRUE)</code></pre>
</div>
</div>
<div id="step-5---load-up-the-weights-into-an-ecoder-model-and-predict" class="section level3">
<h3>Step 5 - Load up the weights into an ecoder model and predict</h3>
<p>We can now specify a brand new model, BUT we can use the layers we have already defined to specify only the input layer and the encoder…</p>
<p>Now we only load up the weights by specifying the by_name argument so that the layer <code>encoder</code> inherits the weights without bothering with the decoder layer.</p>
<pre class="r"><code>encoder_model &lt;- keras_model(inputs = input_layer, outputs = encoder)

encoder_model %&gt;% keras::load_model_weights_hdf5(filepath = &quot;../../static/data/autoencoder_weights.hdf5&quot;,skip_mismatch = TRUE,by_name = TRUE)

encoder_model %&gt;% compile(
  loss=&#39;mean_squared_error&#39;,
  optimizer=&#39;rmsprop&#39;,
  metrics = c(&#39;accuracy&#39;)
)</code></pre>
<div id="compare-to-pca" class="section level4">
<h4>Compare to pca</h4>
<p>OK great! Now we can use the predict function with the new encoder model to see just how this encoder has embeded the higher dimensional data in 2 dimensions:</p>
<pre class="r"><code>embeded_points &lt;- 
  encoder_model %&gt;% 
  keras::predict_on_batch(x = train_X)

embeded_points %&gt;% head</code></pre>
<pre><code>##            [,1]      [,2]
## [1,] -0.9601505 0.7558225
## [2,] -0.8786384 0.6303655
## [3,] -0.9121099 0.6805599
## [4,] -0.8676041 0.6023958
## [5,] -0.9721136 0.7639325
## [6,] -0.9123054 0.6661141</code></pre>
<p>To compare this we also create a pca using the caret package:</p>
<pre class="r"><code>pre_process &lt;- caret::preProcess(train_X,method = &quot;pca&quot;,pcaComp = 2)


pca &lt;- predict(pre_process,train_X)

pca %&gt;% head</code></pre>
<pre><code>##         PC1        PC2
## 1 -2.235619 -0.5961512
## 2 -2.073603  0.6011070
## 3 -2.356444  0.2367943
## 4 -2.296829  0.4978105
## 5 -2.360345 -0.7789828
## 7 -2.433067 -0.1813137</code></pre>
<pre class="r"><code>Viz_data_encoded &lt;- 
  dplyr::bind_rows(
  pca %&gt;% 
    tibble::as_tibble() %&gt;% 
    setNames(c(&quot;dim_1&quot;,&quot;dim_2&quot;)) %&gt;% 
    dplyr::mutate(data_origin = &quot;pca&quot;),
  embeded_points %&gt;% 
    tibble::as_tibble() %&gt;% 
    setNames(c(&quot;dim_1&quot;,&quot;dim_2&quot;)) %&gt;% 
    dplyr::mutate(data_origin = &quot;embeded_points&quot;)
  )

Viz_data_encoded %&gt;% 
  ggplot(aes(dim_1,dim_2, color = data_origin))+
  geom_point()</code></pre>
<p><img src="../../posts/Building_an_autoencoder_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>We can tell that the PCA and encoders produce VERY different results… But we should remember that the purpose of the encoder and it’s loss function was to embed the information into lower dimensions - so ultimately we should compare predictive properties…</p>
</div>
<div id="measure-prediction-accuracy" class="section level4">
<h4>Measure prediction accuracy</h4>
<pre class="r"><code>benchmark &lt;- 
Viz_data_encoded %&gt;%
  mutate(Species = train$Species %&gt;% rep(times = 2)) %&gt;% 
  group_by(data_origin) %&gt;% 
  nest() %&gt;% 
  # mutate(model_lm = data %&gt;% map(glm,formula = Species~., family = binomial())) %&gt;% 
  # mutate(performance = model_lm %&gt;% map(broom::augment)) %&gt;% 
  # unnest(performance,.drop = FALSE)
  mutate(model_caret = data %&gt;% map(~caret::train(form = Species~.,data = .x,method = &quot;rf&quot;))) </code></pre>
<pre><code>## note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre><code>## note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .</code></pre>
<pre class="r"><code>benchmark</code></pre>
<pre><code>## # A tibble: 2 x 3
##   data_origin    data               model_caret
##   &lt;chr&gt;          &lt;list&gt;             &lt;list&gt;     
## 1 pca            &lt;tibble [120 × 3]&gt; &lt;S3: train&gt;
## 2 embeded_points &lt;tibble [120 × 3]&gt; &lt;S3: train&gt;</code></pre>
<pre class="r"><code>for(i in seq_along(benchmark$model_caret)){
  print(benchmark$data_origin[[i]])
  print(benchmark$model_caret[[i]])
}</code></pre>
<pre><code>## [1] &quot;pca&quot;
## Random Forest 
## 
## 120 samples
##   2 predictors
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 120, 120, 120, 120, 120, 120, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.9300934  0.8936083
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 2
## [1] &quot;embeded_points&quot;
## Random Forest 
## 
## 120 samples
##   2 predictors
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 120, 120, 120, 120, 120, 120, ... 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.9503736  0.9243456
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 2</code></pre>
<p>Absolutely crazy…</p>
<p>The embedding done by the encoder was clearly more useful for actual prediction, even though the encoded metrics seemed very lossy. My interpretation of this is that the encoder kept only those features it believed important to represent the original data linearly as apposed to the pca’s loss function which tried to optimize the principle components for explaining variability…</p>
<p>In summary the autoencoder for the iris dataset believes after training that most of the variation in the imput dimensions are just ‘noise’ and that the lossy linear reconstruction is actually better at predicting Species than a standard PCA performed by the caret package?</p>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<ul>
<li>Encoders do well if we are preparing high dimensional data for some regression model or deep learning model.<br />
</li>
<li>Encoders cannot replace dimensionality reduction techniques for visualizing high dimensional features unless a very well defined loss function is used together with a well tuned network<br />
</li>
<li>Encoders are very good at reconstructing data without “noise”</li>
</ul>
</div>
</div>

</div>
            <div class="footer">
                <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>


        <h2>Comments</h2>
        <div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://statslab.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

            </div>
        </div>
        
                
    </body>
</html>
