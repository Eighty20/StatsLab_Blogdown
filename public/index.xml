<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 13 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Condensed R For Data Science: Data Visualisation</title>
      <link>/posts/data_visualisation/</link>
      <pubDate>Fri, 13 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data_visualisation/</guid>
      <description>This piece is part of a series that serves as a condensed help guide that I use to explore R and the tidyverse packages as I work through R for Data Science available here
First, we install/load the relevant packages by installing the tidyverse. This “package” effectively contains all packages developed by the Hadley Wickham:
library(&amp;quot;tidyverse&amp;quot;)## -- Attaching packages -------------------------------------------------------------------------------------------------------------------------- tidyverse 1.2.1 --## v ggplot2 2.2.1 v purrr 0.</description>
    </item>
    
    <item>
      <title>Test multiple sklearn models with LIME</title>
      <link>/posts/test_multiple_sklearn_models_blog/</link>
      <pubDate>Wed, 11 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/test_multiple_sklearn_models_blog/</guid>
      <description>In this blog I load the toy dataset for detecting breast cancer. Often before you can fine tune and productionize any model you will have to play with and test a wide range of models.
Luckily we have frameworks like scikit_learn and caret to train many different models! Prepare yourself
import numpy import pandas import matplotlib.pyplot as plt from sklearn import model_selection from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.</description>
    </item>
    
    <item>
      <title>8020 Hogwarts Sorting Hat!</title>
      <link>/posts/ai_sorting_hat/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/ai_sorting_hat/</guid>
      <description>Back storyLoad up the dataSummarise dataAdd some useful labelsDistribution of affinity to each houseNumber of people in each houseCross tabulation of each person’s top house VS desired houseCross tabulation of each person’s top house VS expected houseExplorationCorrelations between houses predictedCorrelations between top house and desired houseCorrelations between top house and expected houseVisualize similarity of peopleArtificial intelligent sorting hat!</description>
    </item>
    
    <item>
      <title>Explaining machine learning models</title>
      <link>/posts/dalex_explainers/</link>
      <pubDate>Thu, 05 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/dalex_explainers/</guid>
      <description>OverviewThe dataThe problemBenchmark many models with caretSet crossvalidation parametersBuild model data frameworkTrain modelsVisualize the residualsIntroducing DALEX explainers!Model performanceVariable ImportanceVariable responsePrediction breakdownPackageslibrary(tidyverse)library(caret)library(magrittr)library(DALEX)OverviewThis blog will cover DALEX explainers. These are very useful when we need to validate a model or explain why a model made the prediction it made on an observation basis.</description>
    </item>
    
    <item>
      <title>Cats vs Dogs classifier</title>
      <link>/posts/cats_vs_dogs_classifier/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/cats_vs_dogs_classifier/</guid>
      <description>OverviewDownload dataBuild networkData preprocessingImage data augmentationHow it worksCreate new networkTrain new network with augmentation generatorsFurther optimizationTransfer learning - VGG16OverviewDeep neural networks using convulotional layers are currently (2018) the best immage classification algorithms out there… Let’s build one to see what its all about.
How about identifying cats and dogs?
This post follows through the example in the book “Deep learning with R” by Francios Chollet with J.</description>
    </item>
    
    <item>
      <title>IMDB movie classification</title>
      <link>/posts/imdb_movie_classification/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/imdb_movie_classification/</guid>
      <description>OverviewLoad the IMDB dataView dataLink the original dataPrepare data as tensorsOne-hot encodeSet outcome data typesBuild networkSplit test/trainTrainIf we train for only 4 epochsIf we use dropoutInvestigate the best predicted movieOverviewThis post follows through the example in the book “Deep learning with R” by Francios Chollet with J. J. Alaire.</description>
    </item>
    
    <item>
      <title>Finding geographical points of interest using Python</title>
      <link>/posts/finding_geographical_points_of_interest_using_python/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/finding_geographical_points_of_interest_using_python/</guid>
      <description>This blog will take a look at scraping the TomTom and Google Places APIs to get all the points of interest in an area. A recursive grid search algorithm is discussed that efficiently identifies all of the POIs in a large area where there is a limit on the number of results the API returns. 
TomTom vs Google First, let&amp;rsquo;s compare each API:
    TomTom Google Places     Max free daily requests 2500 2500   Max results returned 100 20   Point search Yes Yes   Max point search radius none 50km   Rectangle search Yes No   Up-to-date No Yes    In each case, you need to register for your own API key which you include as a parameter in the search.</description>
    </item>
    
    <item>
      <title>Office R Blog</title>
      <link>/posts/office_r/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/office_r/</guid>
      <description>Prelude and set upPre-reqsStuff for the deckActual OfficeRPrelude and set upIn this markdown I will explain how to use OfficeR to generate powerpoint presentations.
The rmsfuns package was useful just to use the ‘load_pkg’ function which makes it easier to load multiple packages. Firstly, you need the OfficeR package. I am also using the extrafont package in order to use Tahoma in the powerpoint, according to the Eighty20 template.</description>
    </item>
    
    <item>
      <title>Predict house prices - deep learning, keras</title>
      <link>/posts/predict_house_prices_dnn/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/predict_house_prices_dnn/</guid>
      <description>OverviewNaive model (no time index)Load the dataScale the variablesDefine the modelMeasuring over-fit using k-folds crossvalidationGet resultsBenchmark vs Gradient boosting machinesTime series models using LSTM together with an inference networkRead in the dataRead in the dataProcess dataDesign inference modelDesign LSTM modelTest a LSTM modelEverything set… time to get started!Back test LSTM modelCombine LSTM and Inference networks into 1 deep neural network?</description>
    </item>
    
    <item>
      <title>Let&#39;s play with autoencoders (Keras, R)</title>
      <link>/posts/autoencoders_keras_r/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/autoencoders_keras_r/</guid>
      <description>What are autoencoders?How do we build them?Build itStep 1 - load and prepare the dataStep 2 - define the encoder and decoderStep 3 - compile and train the autoencoderStep 4 - Extract the weights of the encoderStep 5 - Load up the weights into an ecoder model and predictConclusionlibrary(ggplot2)library(keras)library(tidyverse)## ── Attaching packages ──────────────────────────────────────────────────── tidyverse 1.</description>
    </item>
    
    <item>
      <title>Serving a machine learning model via API</title>
      <link>/posts/testing_apis_in_r/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/testing_apis_in_r/</guid>
      <description>About the Plumer packageIn oder to serve our API we will make use of the great Plumer package in R
To read more about this package go to:
https://www.rplumber.io/docs/
SetupLoad in some packages.
If you are going to host the api on a suse or redhat linux server make sure you have all the dependencies as well as the packages installed to follow through this example yourself.</description>
    </item>
    
    <item>
      <title>Blogging from a Python Jupyter Notebook</title>
      <link>/posts/blogging_from_jupyter/</link>
      <pubDate>Fri, 30 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/blogging_from_jupyter/</guid>
      <description>This is a follow on post to Stefan&amp;rsquo;s original to show how to generate a blog post from a Jupyter Notebook instead of an R markdown. This post itself started off life as a Jupyter Notebook which lives in the same content/posts folder as the other Rmd files used for the site. We&amp;rsquo;ll walk through how it became a blog post.
The process is a little more complicated than for the Rmd files (since that&amp;rsquo;s what Blogdown was built for but we can still get it to work relatively easily.</description>
    </item>
    
    <item>
      <title>How to add a blog to Blogdown</title>
      <link>/posts/how_to_add_a_blog/</link>
      <pubDate>Thu, 29 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/how_to_add_a_blog/</guid>
      <description>Pre-requisitesCreate from scratchClone or open the blog repository from githubCreate a post using “Addins” dropdownChange/check your date formatWrite a kickass postCompile your new work using BlogdownPush your post to the websiteCreate from existing RmdPre-requisitesInstall blogdown and Hugo in R-Console
https://bookdown.org/yihui/blogdown/installation.htmlCreate from scratchCreating it from scratch is probably the easiest since you can run and test your code as you type it up.</description>
    </item>
    
    <item>
      <title>Dealing with nested data</title>
      <link>/posts/using_e-sports_api/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/using_e-sports_api/</guid>
      <description>Dealing with nested data can be really frustrating…
Especially if you want to keep your workspace nice and tidy with all your data in tables!
With no actual experience trying to get at these nested tibbles can seem almost impossible:
via GIPHY
--Downloading data from an api created by BlizzardTo illustrate how you would deal with nested data I found an api that let’s you download all kinds of data on the e-sport/game called Overwatch.</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>/posts/data-wrangling/</link>
      <pubDate>Tue, 27 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-wrangling/</guid>
      <description>Why data-wranglingIf you can wrangle data into the proper form you can do anything with it…
Data-wrangling is absolutely essential for every data science task where we need to work with collected data.
A recent article from the New York Times said “Data scientists, according to interviews and expert estimates, spend from 50 percent to 80 percent of their time mired in the mundane labor of collecting and preparing data, before it can be explored for useful information.</description>
    </item>
    
    <item>
      <title>Data Science Workflow</title>
      <link>/posts/data-science-workflow/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-science-workflow/</guid>
      <description>Why it mattersWhen working with recurring clients and projects together with tight deadlines it is easy to cut corners or forget good practice.
Here are the top reasons why a data science workflow is vital:
Work is well documented
Folders and files logically are structured
Projects remain version controlled
Data output is tracked
Logical seperation of scripts make productizing easy
Good naming conventions make tracking of the project flow easy</description>
    </item>
    
    <item>
      <title>Benchmarking machine learning models in parallel</title>
      <link>/posts/benchmarking_in_parallel/</link>
      <pubDate>Wed, 15 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/benchmarking_in_parallel/</guid>
      <description>OverviewHaving just started playing with deeplearning models in R, I wanted to visually compare them to other more traditional ML workflows. Of course, deeplearning is generally used where other models fail, but with no need for feature selection and rapidly increasing power and ease of use they may just evolve into a general learning paradigm.
However, with tabular data and packages like caret the machine learning methods have become so streamlined that minimal user input is required at all.</description>
    </item>
    
    <item>
      <title>Design and Analysis of Experiments with R</title>
      <link>/posts/design_and_analysis_of_experiments_with_r/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/design_and_analysis_of_experiments_with_r/</guid>
      <description>IntroductionThis document will cover in depth the use of DoE experiments in R! We will focus our attention to measuring treatment effects, not variances.
Based on the work:
Lawson, John. Design and Analysis of Experiments with R. Chapman and Hall/CRC, 20141217. VitalBook file.
DefinitionsExperiment (also called a Run) is an action where the experimenter changes at least one of the variables being studied and then observes the effect of his or her actions(s).</description>
    </item>
    
    <item>
      <title>A Soft Introduction to Machine Learning</title>
      <link>/posts/intro-machine-learning/</link>
      <pubDate>Fri, 06 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/posts/intro-machine-learning/</guid>
      <description>Important machine learning librariesFuture additionsPart 1 - Data PreprocessingPart 2 - RegressionSimple Linear RegressionMultiple linear regressionPolynomial RegressionSupport vector machine regressionRegression TreesRandom forest regressionA more robust application of machine learning regressions (random forest)Part 3 - ClusteringK-meansPart 4 - Dimensionality ReductionCreate PCAPart 5 - Reinforced LearningMulti-Armed Bandit ProblemUpper Confidence Bound (UCB) methodImprove results using UCBVisualize the model add selectionPart 6 - Parameter Grid Search, Cross-validation and BoostingGrid Search and Parameter TuningXGBoostImportant machine learning librariesI created this document to serve as an easy introduction to basic machine learning methods.</description>
    </item>
    
    <item>
      <title>About Hugo</title>
      <link>/about/</link>
      <pubDate>Wed, 09 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>Hugo is a static site engine written in Go.
It makes use of a variety of open source projects including:
 Cobra Viper J Walter Weatherman Cast  Learn more and contribute on GitHub.</description>
    </item>
    
  </channel>
</rss>