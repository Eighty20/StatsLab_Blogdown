<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep_learning on </title>
    <link>/tags/deep_learning/</link>
    <description>Recent content in Deep_learning on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 03 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/deep_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>IMDB movie classification</title>
      <link>/posts/imdb_movie_classification/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/imdb_movie_classification/</guid>
      <description>OverviewLoad the IMDB dataView dataLink the original dataPrepare data as tensorsOne-hot encodeSet outcome data typesBuild networkSplit test/trainTrainIf we train for only 4 epochsIf we use dropoutInvestigate the best predicted movieOverviewThis post follows through the example in the book “Deep learning with R” by Francios Chollet with J. J. Alaire.</description>
    </item>
    
    <item>
      <title>Predict house prices - deep learning, keras</title>
      <link>/posts/predict_house_prices_dnn/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/predict_house_prices_dnn/</guid>
      <description>OverviewNaive model (no time index)Load the dataScale the variablesDefine the modelMeasuring over-fit using k-folds crossvalidationGet resultsBenchmark vs Gradient boosting machinesTime series models using LSTM together with an inference networkRead in the dataRead in the dataProcess dataDesign inference modelDesign LSTM modelTest a LSTM modelEverything set… time to get started!Back test LSTM modelCombine LSTM and Inference networks into 1 deep neural network?</description>
    </item>
    
    <item>
      <title>Let&#39;s play with autoencoders (Keras, R)</title>
      <link>/posts/autoencoders_keras_r/</link>
      <pubDate>Tue, 17 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/posts/autoencoders_keras_r/</guid>
      <description>What are autoencoders?How do we build them?Build itStep 1 - load and prepare the dataStep 2 - define the encoder and decoderStep 3 - compile and train the autoencoderStep 4 - Extract the weights of the encoderStep 5 - Load up the weights into an ecoder model and predictConclusionlibrary(ggplot2)library(keras)library(tidyverse)## ── Attaching packages ──────────────────────────────────────────────────── tidyverse 1.</description>
    </item>
    
  </channel>
</rss>